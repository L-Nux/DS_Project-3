# -*- coding: utf-8 -*-
"""multilabel_classification_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jsfO2-EX__17yEwHCwc5YdAuVwaPn4gw
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
#to include graphs next to the code
#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='final++.csv' #load the final dataset with all features for building NN
df = pd.read_csv(file)
print(df.describe()) #statistics

# drop redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)
df

"""Why do we need NN? Define Target column Y for predictions

Currently we start with the dataset that has 29 features (including generated and aligned ones from other incorporated datasets).

Would like to predict by NN what transport modes would be simultaneously involved in a trip path for the traveller (how many) to help him to decide in advance would it be comfortable or not (e.g with respect to safety - covid19, risk of usage a special transport type) to even start a trip

We do not need all unique&specific combinations, specifing the order of how many trains did we use per trip here, we just need to know that for that path we used a train or a car (or the combination of them for the customer)=> how many modes (labels) would be involved in journey

For this reason, we will adapt the column with labels
"""

#investigate all unique values
for c in df:
    print(df[c].unique())
    print(df[c].nunique())

"""The initial broadness of ranges was preserved in the source dataset"""

dff=df.copy()#take a copy

"""Initially, we have 33 unique values in the column "finalsolutionusedlabels"(all the range)-we will trasnsform the values, for this reason we make a copy of df"""

#replace the values

dff['finalsolutionusedlabels'] = dff['finalsolutionusedlabels'].replace(['[db_fv, blablacar]','[db_fv, blablacar, db_fv]'],'[blablacar, db_fv]')
dff['finalsolutionusedlabels'] = dff['finalsolutionusedlabels'].replace(['[db_fv, flixbus]','[db_fv, flixbus, db_fv]','[db_fv, flixbus, db_fv, flixbus]','[db_fv, flixbus, db_fv, flixbus, db_fv]','[flixbus, db_fv, flixbus, db_fv]','[flixbus, db_fv, flixbus]'],'[flixbus, db_fv]')
dff['finalsolutionusedlabels'] = dff['finalsolutionusedlabels'].replace(['[flixbus, blablacar, db_fv]','[db_fv, blablacar, flixbus]','[db_fv, blablacar, flixbus, db_fv]','[blablacar, db_fv, flixbus]','[blablacar, flixbus, db_fv]','[db_fv, flixbus, blablacar, flixbus]','[flixbus, blablacar, db_fv, flixbus]','[flixbus, db_fv, blablacar]','[flixbus, blablacar, flixbus, db_fv]','[blablacar, db_fv, flixbus, db_fv]','[db_fv, blablacar, db_fv, flixbus]'],'[db_fv, flixbus, blablacar]')
dff['finalsolutionusedlabels'] = dff['finalsolutionusedlabels'].replace(['[flixbus, blablacar]','[flixbus, blablacar, flixbus]'],'[blablacar, flixbus]')
dff['finalsolutionusedlabels'] = dff['finalsolutionusedlabels'].replace(['[flixbus, flight, db_fv, flixbus]','[db_fv, flight, flixbus, db_fv]'],'[flixbus, flight, db_fv]')
dff['finalsolutionusedlabels'] = dff['finalsolutionusedlabels'].replace(['[flixbus, flight, flixbus]'],'[flixbus, flight]')

"""Changing, for example,an instance: [db_fv, blablacar, db_fv] to [blablacar, db_fv], since we only need to know the different modes involved for further classification, and not how many separate trains, buses or their specific order"""

#check the uniqness
uniqueValues1 = dff['finalsolutionusedlabels'].unique() #enumeration
uniqueValues2 = dff['finalsolutionusedlabels'].nunique() #count
print (uniqueValues1)
print (uniqueValues2)

"""From 33 combinations to 10 to see for the customer which transport type would be involved in trip in general- this would be multilabel classification problem (more than 2 outcomes where the classes are not mutually exclusive- train could be not only in 1 sample with labels enumeration)- 5 different and separate labels wrt to transport mode involved"""

#rename the target column
dff.rename(columns={'finalsolutionusedlabels': 'modelabel'}, inplace=True)

"""Had changed the values, changed the name for convenience"""

#calculate the percentage of samples per target column 'modelabel'
s = dff.modelabel
counts = s.value_counts()
percent = s.value_counts(normalize=True)
percent100 = s.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'
pd.DataFrame({'counts': counts, 'per': percent, 'per100': percent100})

"""Dataset is highly unbalanced with regards to classes that are not mutually exclusive and contain same mode labels inside,this is boosting the complexity of the problem: some mode combinations are very rare samplewise (4 samples only), some are extremely popular-34 thousands of records for the train in the datatset with 85 thousands rows- this addresses the "data imbalance" problem

Total-85153 sample routes

Also the 2d problem is the involvement of the label in class itself:

(db_fv)train- 65744 records contain the label train - 77,2% of df-postitive(presence); 85153-65744=19409 negative (absence)

flixbus-32979 records 38,7% of df positive; 52 172 rows negative

blablacar-18028 records 21,2% of df; 67125 rows negative

car-6030 rows 7,08 of ds; 79123 rows negative

flight-178 records, - again only 0,2% ; 84975 rows negative(the majority)

total:122 959 with the presence of the labels (positive)

total:302 804 the absence of the labels (negative)

total toal= 425763

https://stackoverflow.com/questions/60366175/how-to-get-sample-weights-and-class-weights-for-multi-label-classification-probl

the question posted here summarizes the challenge, however no answers were received. 


"""

#calculate the percentage of values per column 'multimodality'
m = dff.multimodality
counts = m.value_counts()
percent = m.value_counts(normalize=True)
percent100 = m.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'
pd.DataFrame({'counts': counts, 'per': percent, 'per100': percent100})

"""considering another binary feature with only 2 classes within the column are also a bit unbalanced (just in case if we want to set another target in the future)

Conclusion: The DS with respect to 'modelabel' column is unbalanced with the records, plain train would take 40% of labels,flixbus+train-22%, then we see that the records are dramatically decreasing, for the combination of flixbus and flight we have only 4 records in comparison to train where we have 34430 records, as well as the class and labels imbalance itself (label train is super common one and its present in 77& of records, at the same time label car is only in 7,1% of dataset with the flight for only 0,2 of the rows.

Hence, there is a need to figure out how to tackle this obstacle for NN training to prevent overfitting:

-Oversampling before running NN alghorithm SMOTE (only for the training set- however it could solve the data imbalance but won't be solving the class imbalance problem(presence of the labels);

-Playing with weights (sample weights_class weights) on a running stage (biased weights for the minor classes or balancing the weights)- for multilabel problem we need to tune the sample weights.

Before running the NN alghorithm we would like to process more our dataset- currently there are no missing values there, we need to one hot encode the X data categorical columns and apply the multilabel encoding for the target Y.

will have a look on the correlation matrix again wrt to numerical features:

RELU function, that we will use for NN, could deal with outliers if NN is deep enough- we will try to compile a model without the reduction of extreme outliers

Usage of the possible features(as many as possible), but dropping the categorical ones with respect to population (we will proceed with the numerical representation of these features-population per OD cities) and City Names (we have numerical distance feature that could be a nice replacement&summary)=> this would be done to not extend that much the dataset after the one-hot-encoding

https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18
try oversampling AFTER train test split on training set only
"""

dff.drop(dff.columns[[9,10, 12, 13]], axis = 1, inplace = True) #remove columns

"""Getting rid of numerical 'travelfrom', 'travelto', since we have categorical and numerical onces wrt to population size( we will use numerical ones), get rid of 2 categoricals with OD names wrt to cities- having them and encoding via one-hot- we extending the size of inputs dramatically (82 columns), removing them-we have 31 with all encoding for X data"""

dff#check

over=dff.copy() #a copy of data for oversampling the minority

#check and drop duplicates
dff.drop_duplicates(keep='last', inplace=True)
#no duplicates

show=dff.corr(method='kendall')#pairwise correlation
plt.figure(figsize=(10,10))
sns.heatmap(show,annot=True,cmap='PuOr')

"""We may consider to drop in the future 'calories_burnt', also earnings_gross since the correlations with the previous feature-totaltraveltimeinhours is almost 1- the new features provide little new information, we could also save it and run the NN with all of them preserved

A low correlation between the new feature and existing features is likely preferable.

A strong linear correlation between the new feature and the predicted variable is an good sign that a new feature will be valuable for NN, but the absence of a high correlation is NOT necessary a sign of a poor feature, because NN are not restricted to linear combinations of variables.

Strong correlation between calories_burnt and earnings_gross (also earnings correlate with the distance)

Strong correlation of the delay_probability with multimodality => the delay got computed based on transportation mode

The beauty of neural networks is that little feature engineering and preprocessing is required - features are instead learned by intermediate layers.References
https://datascience.stackexchange.com/questions/716/how-to-choose-the-features-for-a-neural-network
"""

dff.modelabel.dtype #type object

#copies of dataframe for further usage
extr= dff.copy()

#copies of dataframe
parse=dff.copy()

"""We need to create a new one column for target via mapping with the previous one (right now string values in the column)=> making it as an array for the data types to process it with MultilabelBinarizer(the input should be array of arrays)

The challenge was to force the MLB working and correct the datatypes, to overcome it also the manual binarizer was implemented on artificial piece of dataset, however it was not working on real df, only after changing the datatypes properly the MLB worked.
"""

#create matrix for 0 and 1- multilabel binarizer

n = 85153
classes = ['flixbus', 'db_fv', 'blablacar', 'car', 'flight'] #5 unique classes -'labels'
aa = pd.DataFrame(0, index=np.arange(n), columns=classes)
aa

#test on trial dataset
n = 10
test = pd.DataFrame(0, index=np.arange(n), columns=['modelabel'])
test['modelabel'][0] = ['flixbus']
test['modelabel'][1] = ['flixbus', 'db_fv']
test['modelabel'][2] = ['car']
test['modelabel'][3] = ['car', 'flight']
test['modelabel'][4] = ['db_fv', 'flixbus']
test['modelabel'][5] = ['blablacar', 'flixbus']
test['modelabel'][6] = ['blablacar', 'flight']
test['modelabel'][7] = ['flixbus']
test['modelabel'][8] = ['car']
test['modelabel'][9] = ['flight']
test

#test on trial dataset the cycle
for r in range(0, len(test)):#iterate
  for c in range(0, len(test['modelabel'][r])):
    aa[test['modelabel'][r][c]][r] = 1
aa

"""The manual binarizer worked on dummy datatset only since it was already 'array' in the label column
For the actual dataset the function eventually got implemented,
the 'parse' df would be responsible to change the data type in the target column
"""

parse['modelabel_final'] = 0 #create new column for target with correct datatype isnside

#function to correct the values for the target column
def parse_modelabel(modelabel):
  modelabel = modelabel.strip("[]")#remove the brackets
  modelabel = modelabel.replace(" ", "") #replace the redundant spaces
  modelabel = modelabel.split(',')#make array from the string by comma
  return modelabel

#fill out the values row by row
for row in range(0, len(parse)):
  parse['modelabel_final'][row] = parse_modelabel(parse['modelabel'][row])#alignment

parse#examine the newest column with values

#encode target Y column
from sklearn.preprocessing import MultiLabelBinarizer #tool for encoding

multilabel_binarizer = MultiLabelBinarizer()
multilabel_binarizer.fit(parse['modelabel_final']) #fit binarizer- works only with array of arrays being transmitted
labels = multilabel_binarizer.classes_
labels #return 5 unique labels per transport mode- 'blablacar', 'car', 'db_fv', 'flight', 'flixbus'

"""finally worked with the correct dtypes of array per target"""

#transform y target column
y = multilabel_binarizer.transform(parse['modelabel_final']) #have to be array of arrays

"""for the 1st approach ith the first model it was decided to apply binarizer before train-validation-test split (however its not a nice way=> we need to fully prevent the data leakage always), since the classes and samples are imbalanced without boosting the minority and with that we didnt want to have 4 classes in train/test/or validation set instead of all 5 (since after the splitting 178 records with flight label may be concentrated in only 1 splitted set out of 3)
X data would be one hot encoded after train test split appropriately
"""

y # 5 unique classes got encoded inside

#ov=dff.copy() #for generation of synthetic values
ov=parse.copy() for 2d modelling
#over=parse.copy() #for generation of synthetic values

over=parse.copy()#for 3d modelling

ov.drop(["modelabel"], axis=1)#drop not correct target

#drop target column from dataframe, define data X
X = extr.drop(["modelabel"], axis=1) # we use previous copy of df, not the parsed one

X.drop(dff.columns[[16,17,18,19,21,22]], axis = 1, inplace = True) #drop redundant columns

"""Will not involve "temeratureto", "humidityto","pressureto" as well as "from", since these columns got used for the 'mood_upgrade' feature creation & calculation only- these features are not interesting separately"""

X#check the results
#18 input features

#library for splitting
from sklearn.model_selection import train_test_split

#X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7, random_state=1) #train-remaining split #rs-42
#tried different sizes and seeds
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.6, random_state=3) #size 0.5 #rs-1, rs-27, rs-13,rs-42,rs-3= find rs to get the support for the flight class
#check if I have flight in training data for y

"""To overcome the data leakage problem -do train-validation-test splits before one-hot encoding of categorical features and data normalization

Future unseen data will be processed in exact same way as the testing data, thus ensures consistency in model performance.references
https://towardsdatascience.com/avoid-data-leakage-split-your-data-before-processing-a7f172632b00
"""

#test-validation split
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5,random_state=1)

print(X_train.shape), print(y_train.shape)
print(X_valid.shape), print(y_valid.shape)
print(X_test.shape), print(y_test.shape)

X_test#check

#function to encode all categorical columns via one-hot but with k-1 to det rid of 

def encode(data):
  categorical_col= ['objective',	'finiteautomaton',	'consideredpreferences', 'stresslevel', 'mood_upgrade']#define columns
  datanew = pd.get_dummies(data, columns=categorical_col)#convert categorical columns
  return datanew

X_train=encode(X_train) 
X_test=encode(X_test)
X_valid=encode(X_valid)

"""'get dummies' is identical to one hot encoding for categorical variables (k-1) The dummy encoding is a small improvement over one-hot-encoding. Dummy encoding uses N-1 features to represent N labels/categories.

One-Hot encoding does not suffer from the exponential curse of dimensionality, since the different sub-features are fixed all together. Once we set 1 in one sub-feature all others are automatically set to 0. Permutation of values increase linearly and not exponentialy

One-hot encoding works better for NN than label encoding

Get dummies helps to avoid multicollinearity trap , we drop it specifically by "N-1"
"""

#check what we have gotten as the result
X_valid

"""from 18 to 31 columns with once hot encoded features"""

#normalize the training set via fit_transform
from sklearn.preprocessing import MinMaxScaler #since the distribution for the numerical is not normal- we use min max approach

norm = MinMaxScaler()
train_X_norm = norm.fit_transform(X_train) #use fit_trasform for test data
train_X_norm

"""To normalize over the whole dataset -the bad outcome to introduce future information into the training explanatory variables 

Therefore, we should perform feature scaling over the training data. 
Then perform normalisation on testing /validation instances as well, but this time using the fit of training explanatory variables. 
In this way, we can test and evaluate whether our model can generalize well to new, unseen data points.
"""

#normalize validation data
valid_X_norm = norm.transform(X_valid)#use only transform for validation set
valid_X_norm

#normalize testing data
test_X_norm = norm.transform(X_test)#use only transform for test set
test_X_norm

#1st model to balance imbalanced labels in ds with multilabel classes inside
from sklearn.utils import class_weight
sample_weights = class_weight.compute_sample_weight(class_weight="balanced", y=y_train) # balancing sample weights for multilabel problem

""""compute_class_weights" can be used for multiclass classifications, but apparently not for multi-label problems as we have

For this reason, tried using "compute_sample_weight" from "class_weigt" instead with balancing approach for imbalanced classes/ labels, since 1 sample is belonging to >than 1 class, could handle multi-label output problems

references https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html

Scenarios where we usually use class weights-

When the data contains an imbalanced number of classes. When some classes need more attention in some scenarios even with a balanced data set. When we consider the F1 score as a more important metric than Accuracy.(our case could be)

Scenarios where we usually use sample weights-

When some samples need more attention according to time and characteristics. When we believe that giving priority to the latest or oldest samples may increase the accuracy of the model. When the model is required to adapt quickly to data generated at the latest time period. When we believe that the real information in training data is segregated only to a fewer number of samples.

auc, roc references also
https://medium.com/the-owl/imbalanced-multilabel-image-classification-using-keras-fbd8c60d7a4b
"""

#preparation for multilabel classification with Deep Learning

from sklearn.datasets import make_multilabel_classification
from keras.models import Sequential #to construct the model
from keras.layers import Dense
from sklearn.metrics import accuracy_score

from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from keras.layers import Dropout



n_features=31 # input columns
n_classes=5 # classes

batch_size= 200 #tried different sizes, the most suitable one wrt to the results
n_epochs= 50 #tried also other combinations
verbosity=1


callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5) #goal is min loss-difference between predicted and actual
#the callback will stop the training when there is no improvement in the loss for 5 consecutive epochs

# create the 1st model

model2 = Sequential()
model2.add(Dense(62, activation='relu', input_dim=n_features)) #31*2
model2.add(Dropout(0.2)) #using dropout on hidden layers
model2.add(Dense(31, activation='relu'))
model2.add(Dropout(0.2))
model2.add(Dense(15, activation='relu'))
model2.add(Dropout(0.2))
model2.add(Dense(8, activation='relu')) #make the dense thinner
model2.add(Dropout(0.2))
model2.add(Dense(n_classes, activation='sigmoid')) #sigmoid in output to handle multilabel classification problems

# compile the model
model2.compile(loss=binary_crossentropy,
              optimizer=Adam(learning_rate=0.01), #tried learning rate=0.000001=the result got worse, use Adam
              #metrics=['categorical_accuracy']
              #metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()]) #for binary_accuracy =0,89
              metrics = [tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()])
              #metrics=[tf.keras.metrics.Precision()])

model2.summary() #add the data about the model

# fit data to model
history = model2.fit(train_X_norm, y_train,
          sample_weight=sample_weights, #align with the parameters
          batch_size=batch_size,
          epochs=n_epochs,
          verbose=verbosity,
          callbacks=[callback],
          validation_data=(valid_X_norm,y_valid), #embedd validation data, that was collected by splitting in advance
          shuffle=True)         

# generate generalization metrics
score = model2.evaluate(test_X_norm, y_test, verbose=0)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}') #categorical 
#using binary metrics-the output is binary accuracy

"""The result is moderate level of test loss 0,06 (as well as validation and of course training) and not perfect but still nice indicator of test accuracy 77,5%

Training accuracy- around 74%, validation accuracy arounf 78%
AUC-0,98-0,99 training-validation

Dropout is a technique, where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.

Using the BinaryAccuracy to calculate the accuracy. That is we are comparing true and predicted label vectors by matching one pair at a time! Thus, the prediction vectors with all 0 or all 1 values can lead to considerable good accuracy. 
https://medium.com/deep-learning-with-keras/how-to-solve-multi-label-classification-problems-in-deep-learning-with-tensorflow-keras-7fb933243595

this line is in code but in commented state, applying the Binary accuracy the result is almost 100% always, 89-90+ depending on a run, decided to proceed with categorical accuracy as was suggested in other sources

'Accuracy' itself is not the best metric to rely on, when evaluating imbalanced datasets (however we tried to balanced the weights), as it can be very misleading. Metrics, that can provide better insight include:

Confusion Matrix: a table showing correct predictions and types of incorrect predictions.

Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.

Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.

F1: Score: the weighted average of precision and recall.
"""

from sklearn.metrics import classification_report #check the classification report
pred = model2.predict(test_X_norm, batch_size=200, verbose=1) #use the test data predictions and select the batch size
predicted = np.argmax(pred, axis=1)
report = classification_report(np.argmax(y_test, axis=1), predicted)#true target values of data +predicted output of model
print(report)

"""Indicators precision, recall and f-1 score for all the labels:

Precision- we got an extremely hight precision of 1-0,99 for the first 3 classes (blablacar,car,db_fv) (correctly predicted positive to the total positive)=>low false positive rate there, however the 3d index-flight- we were always getting 0 for precision and other indicators , also probably by the reason of low support (only 3 values in testing data)
the 5th label flixbus was also having low  precision high false positives indicator;

Recall(sensitivity): eveyrwhere apart from flight label the indicator is >0.5 -a good result

F1-score:F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost( could be in our case, however the client would like to be in a situation of false positive (flight was predicted, but actually we do not have it- in order to foresee the risks anyway) than to have false negative (actually we have a flight but it was predicted that it shouldnt be and a customer would undercalculate his risks) -high everywhere apart from 3d class-0 and 4th-less than 0.5

We will try to change the indicators for 3 and 4th classes by smote smaplewise and labelwise

the 4th class-flight (3 with index) could not be predicted even with balancing the classes weihts_sample weights, since we have only 178 records in total DS, that would contain the label "flight" and they easily could be out of the splitted datasets-in test set as well (we got the support of only 3 records-trying other random seeds-0 or 1 records were involved)

Attemts to try to improve:
1)shuffling the datasets with different random seeds-tried the techniques-ended up here with the most suitable
2) more records for validation and test data (changed from 0.15 to 0.2)
2) SMOTE for balancing-oversampling technique-perform in advance-for the 2d and 3d models below
"""

print(history.history.keys()) #also could add binary accuracy

"""check if we have flight in training"""

from matplotlib import pyplot as plt
#plottig accuracy

plt.plot(history.history['categorical_accuracy'])
plt.plot(history.history['val_categorical_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Number of epochs')
plt.legend(['train', 'val'], loc='lower right')
plt.show()

"""Dramatically increasing with number of epochs until 2,5 then stays +- in th same range"""

#plotting loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Number of epochs')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

"""Steeply decreasing for validation set till2,5 of epochs, for training until 3 epochs, afterwards stays in the same range for training but periodically growing for validation with min below 0.10"""

#plotting auc
plt.plot(history.history['auc_15']) #depending on number of runs
plt.plot(history.history['val_auc_15'])
plt.title('Model AUC')
plt.ylabel('AUC')
plt.xlabel('Number of epochs')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""Area under the ROC curve- AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.
AUC is desirable for the following two reasons:

AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.
AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.
AUC represents the probability that a random positive (green) example is positioned to the right (ranked highly) of a random negative (red) example.
"""

#confusion matrix per label
import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix
y_pred = model2.predict(test_X_norm)
confusion_matrix1 = sklearn.metrics.multilabel_confusion_matrix(y_test, np.rint(y_pred))
confusion_matrix1

"""Nice ratio of true positives and negatives for each label,
the things to keep in mind:

-we would like to reduce False negatives more than False positives
super high ratio of false negatives for flight label (4th)- it was predicted that the flight was not involved but actually it is involved- as a result customer didnt prepare well for the trip
"""

#try sampling techniques
from imblearn.over_sampling import SMOTE #generate the synthetic samples
#library for splitting
from sklearn.model_selection import train_test_split

# separate input features and target
y1 = y
#y1=ov.modelabel_final
X1 = ov.drop('modelabel_final', axis=1)

# setting up testing and training sets
X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.25, random_state=27)

sm = SMOTE(random_state=27, sampling_strategy='not minority') #sampling_strategy works for multilables/multiclass not only for binary
X_train, y_train = sm.fit_resample(X_train, y_train)

"""In accordance to the error: ValueError: Imbalanced-learn currently supports binary, multiclass and binarized encoded multiclasss targets. Multilabel and multioutput targets are not supported.

multilabel problem is not supported for generation the synthetic values via IMBLEARN SMOTE
"""

#try oversampling the minority sample out of 10 samples

#apply only for training dataset after SPLITTING
from sklearn.utils import resample

# Separate input features and target
y1 = ov.modelabel

X1 = ov.drop('modelabel', axis=1)


# setting up testing and training sets
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=3) #rs-27 tried also, rs -42, rs-1

# concatenate training data back together
Xtr = pd.concat([X1_train, y1_train], axis=1)

# separate minority and majority classes
db_fv = Xtr[Xtr.modelabel == '[db_fv]'] #majority class with the highest number of records
flixbusdb_fv=Xtr[Xtr.modelabel == '[flixbus, db_fv]']
blablacardb_fv=Xtr[Xtr.modelabel == '[blablacar, db_fv]']
flixbus=Xtr[Xtr.modelabel == '[flixbus]']
car=Xtr[Xtr.modelabel == '[car]']
db_fvflixbusblablacar=Xtr[Xtr.modelabel == '[db_fv, flixbus, blablacar]']
blablacarflixbus=Xtr[Xtr.modelabel == '[blablacar, flixbus]']
blablacar=Xtr[Xtr.modelabel == '[blablacar]']
flixbusflightdb_fv=Xtr[Xtr.modelabel == '[flixbus, flight, db_fv]']
flixbusflight=Xtr[Xtr.modelabel == '[flixbus, flight]']



# upsample minority
flixbusdb_fv_upsampled = resample(flixbusdb_fv,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results
                          # upsample minority
blablacardb_fv_upsampled = resample(blablacardb_fv,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results
# upsample minority
car_upsampled = resample(car,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results
# upsample minority
flixbus_upsampled = resample(flixbus,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results
# upsample minority
db_fvflixbusblablacar_upsampled = resample(db_fvflixbusblablacar,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results

blablacarflixbus_upsampled = resample(blablacarflixbus,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results

blablacar_upsampled = resample(blablacar,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results   

flixbusflightdb_fv_upsampled = resample(flixbusflightdb_fv,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results

flixbusflight_upsampled = resample(flixbusflight,
                          replace=True, # sample with replacement
                          n_samples=len(db_fv), # match number in majority class
                          random_state=27) # reproducible results                         

# combine majority and upsampled minority
upsampled = pd.concat([db_fv, car_upsampled,flixbus_upsampled,flixbusdb_fv_upsampled,blablacardb_fv_upsampled,db_fvflixbusblablacar_upsampled,blablacarflixbus_upsampled,blablacar_upsampled,flixbusflightdb_fv_upsampled,flixbusflight_upsampled])

# check new class counts
upsampled.modelabel.value_counts()

"""same records per all lines with the pass (samplewise smote)"""

y1_new = ov.modelabel_final #correct values for y1_test!

y1_newtrain, y1_newtest = train_test_split(y1_new, test_size=0.3, random_state=3) #use y_newtest-same split

y1_newtest#use for y1_test multilabel

#encode target Y1 column


multilabel_binarizer = MultiLabelBinarizer()
multilabel_binarizer.fit(upsampled['modelabel_final']) #fit binarizer #ov
labels = multilabel_binarizer.classes_
labels #return 5 unique labels per transport mode- 'blablacar', 'car', 'db_fv', 'flight', 'flixbus'

#transform y target column
y1_train = multilabel_binarizer.transform(upsampled['modelabel_final'])

y1_train

y1_test=multilabel_binarizer.transform(y1_newtest) #align with y1_newtest

y1_test#check

X1_train = upsampled.drop('modelabel', axis=1)

X1_train.drop(X1_train.columns[[15,16,17,18,20,21,24]], axis = 1, inplace = True) #drop redundant columns

X1_train #18 columns

X1_test.drop(X1_test.columns[[15,16,17,18,20,21,24]], axis = 1, inplace = True) #drop redundant columns)
X1_test

#function to encode all categorical columns via one-hot but with k-1 to det rid of multicollinearity trap

def encode(data):
  categorical_col= ['objective',	'finiteautomaton',	'consideredpreferences', 'stresslevel', 'mood_upgrade']#define columns
  datanew = pd.get_dummies(data, columns=categorical_col)#convert categorical columns
  return datanew

X1_train=encode(X1_train) 
X1_test=encode(X1_test)

#normalize the training set via fit_transform
from sklearn.preprocessing import MinMaxScaler #since the distribution for the numerical is not normal- we use min max approach

norm = MinMaxScaler()
train_X1_norm = norm.fit_transform(X1_train) #use fit_trasform for test data
train_X1_norm

#normalize testing data
test_X1_norm = norm.transform(X1_test)#use only transform for test set without fit
test_X1_norm

#to balance imbalanced labels in ds with multilabel classes inside
from sklearn.utils import class_weight
sample_weights1 = class_weight.compute_sample_weight(class_weight="balanced", y=y1_train) # balancing sample weights for multilabel problem

"""tried with and tried without- didnt introduce any special change, decided to switch it off in the model code)"""

#additional model-2d with oversampling of minority samplewise
n_features=31 # input columns
n_classes=5 #classes

batch_size= 200
n_epochs= 50
verbosity=1
validation_split=0.15 #in this case we splitted for train-test only and now we want to have validation data as well for assesment

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5) #goal is min loss-difference between predicted and actual
#the callback will stop the training when there is no improvement in the loss for 5 consecutive epochs

# create the second model

model1 = Sequential()
model1.call = tf.function(model1.call) #for feature importance
model1.add(Dense(62, activation='relu', input_dim=n_features)) #31*2
model1.add(Dropout(0.2)) #using dropout on hidden layers
model1.add(Dense(31, activation='relu'))
model1.add(Dropout(0.2))
model1.add(Dense(15, activation='relu'))
model1.add(Dropout(0.2))
model1.add(Dense(8, activation='relu')) #make the dense thinner
model1.add(Dropout(0.2))
model1.add(Dense(n_classes, activation='sigmoid')) #sigmoid in output to handle multilabel classification problems

# compile the model
model1.compile(loss=binary_crossentropy,
              optimizer=Adam(learning_rate=0.01), #tume learning rate=0.000001=the result got worse
              #metrics=['categorical_accuracy']
              #metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()]) #for binary_accuracy =0,89
              metrics = [tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()])
              #metrics=[tf.keras.metrics.Precision()])

model1.summary() #add the data about the model

# fit data to model
history = model1.fit(train_X1_norm, y1_train,
          #sample_weight=sample_weights1, #switched off
          batch_size=batch_size,
          epochs=n_epochs,
          verbose=verbosity,
          callbacks=[callback],
          validation_split=validation_split,
          shuffle=True)
          


# generate generalization metrics
score = model1.evaluate(test_X1_norm, y1_test, verbose=0)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}') #categorical 
#using binary metrics-the output is binary accuracy

"""Train on 204102 samples, validate on 36018 samples
Accuracy got improved-to 0,86 from 0,78
improving the training accuracy but decresed in validation accuracy, validation auc decreased,training loss increased, val loss decreased

val auc is still high =>almost 1
"""

from sklearn.metrics import classification_report #check the classification report
pred = model1.predict(test_X1_norm, batch_size=200, verbose=1)
predicted = np.argmax(pred, axis=1)
report = classification_report(np.argmax(y1_test, axis=1), predicted)#true target values of data +predicted output of model
print(report)

"""Lower support for flight in test data-again indicators are 0s everywhere

Indicators precision, recall and f-1 score for all the labels:

Precision- we got an extremely hight precision of 1-0,82 for the 4 classes (blablacar,car,db_fv,flixbus) (correctly predicted positive to the total positive)=>low false positive rate there, 
however the 3d index again-flight- we as before were always getting 0 for precision and other indicators , also probably by the reason of low support (only 2 values in testing data)
the 5th label flixbus got improved, the 3d label decreased from 0.99 to 0.82

Recall(sensitivity): decreased for the blablacar label prediction and increased for the train, didnt change for the flight

F1-score:F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost( could be in our case, however the client would like to be in a situation of false positive (flight was predicted, but actually we do not have it- in order to foresee the risks anyway) than to have false negative (actually we have a flight but it was predicted that it shouldnt be and a customer would undercalculate his risks) -high everywhere apart from 3d class-0 and 1st-less than 0.5 blablacar

it was smote technique implemented samplewise
We will try to change the indicators for 3 and 4th classes by smote labelwise
"""

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix
y1_pred = model1.predict(test_X1_norm)
confusion_matrix = sklearn.metrics.multilabel_confusion_matrix(y1_test, np.rint(y1_pred))#create the matrix

confusion_matrix #for labels per each of 5 labels

"""Nice ratio of true positives and negatives for each label, the things to keep in mind:

-we would like to reduce False negatives more than False positives super high ratio of false negatives for flight label was eliminated in comparison to previous model results
"""

def extract_rows_by_type(t, df): #extract rows that contain the word per mode in separate df for class (label)upsamplin the minorities
  df_by_type = df[0:0]
  for index, row in df.iterrows():
    if t in df["modelabel_final"][index]:
      df_by_type = df_by_type.append(row, ignore_index=True)
  return df_by_type
  #technically will create 5 diff datasets and count the record numbers

#try oversampling the minority class out of 5 classes with positive values (presence) in ds
#try 3d aprroach=3d model
from sklearn.utils import resample

# separate input features and target
y2 = over.modelabel_final
X2 = over.drop('modelabel_final', axis=1)

# setting up testing and training sets
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.25, random_state=3) #rs27, rs-42,rs-1 tried

# concatenate training data back together
Xtrr = pd.concat([X2_train, y2_train], axis=1)

# separate minority and majority classes
blablacar = extract_rows_by_type("blablacar", Xtrr) #running time per 1 df creation is about 3min each

blablacar #13526 rows

car = extract_rows_by_type("car", Xtrr)#running time 3 min

car #4546 rows

db_fv = extract_rows_by_type("db_fv", Xtrr)#running time 20 min

db_fv#49324 rows in training set

flight = extract_rows_by_type("flight", Xtrr)# running time is 5 seconds

flight #133 rows

flixbus = extract_rows_by_type("flixbus", Xtrr)#running for 5 min

flixbus #24694 rows

#upsample minority
blablacar1_upsampled = resample(blablacar,
                      replace=True, # sample with replacement
                      n_samples=len(db_fv), # match number in majority class
                      random_state=27) # reproducible results
flixbus1_upsampled = resample(flixbus,
                      replace=True, # sample with replacement
                      n_samples=len(db_fv), # match number in majority class
                      random_state=27) # reproducible results                     
flight1_upsampled = resample(flight,
                      replace=True, # sample with replacement
                      n_samples=len(db_fv), # match number in majority class
                      random_state=27) # reproducible results
car1_upsampled = resample(car,
                      replace=True, # sample with replacement
                      n_samples=len(db_fv), # match number in majority class
                      random_state=27) # reproducible results


# combine majority and upsampled minority
upsamplednew = pd.concat([db_fv, flight1_upsampled,blablacar1_upsampled,flixbus1_upsampled,car1_upsampled])

#  check new class counts
upsamplednew

#encode target Y2 column
multilabel_binarizer = MultiLabelBinarizer()
multilabel_binarizer.fit(upsamplednew['modelabel_final']) #fit binarizer #ov
labels = multilabel_binarizer.classes_
labels #return 5 unique labels per transport mode- 'blablacar', 'car', 'db_fv', 'flight', 'flixbus'

"""as it should be-encoding after splitting =)"""

#transform y target column
y2_train = multilabel_binarizer.transform(upsamplednew['modelabel_final'])

y2_train

y2_test=multilabel_binarizer.transform(y2_test) #align with y1_newtest

y2_test

up=upsamplednew.copy()

X2_train = up.drop('modelabel', axis=1)

X2_train

X2_train.drop(X2_train.columns[[15,16,17,18,20,21,24]], axis = 1, inplace = True) #drop redundant columns

X2_train

X2_test.drop(X2_test.columns[[8,16,17,18,19,21,22]], axis = 1, inplace = True) #drop redundant columns)
X2_test

#function to encode all categorical columns via one-hot but with k-1 to det rid of multicollinearity trap

def encode(data):
  categorical_col= ['objective',	'finiteautomaton',	'consideredpreferences', 'stresslevel', 'mood_upgrade']#define columns
  datanew = pd.get_dummies(data, columns=categorical_col)#convert categorical columns
  return datanew

X2_train=encode(X2_train) 
X2_test=encode(X2_test)

#normalize the training set via fit_transform
from sklearn.preprocessing import MinMaxScaler #since the distribution for the numerical is not normal- we use min max approach

norm = MinMaxScaler()
train_X2_norm = norm.fit_transform(X2_train) #use fit_trasform for test data
train_X2_norm

#normalize testing data
test_X2_norm = norm.transform(X2_test)#use only transform for test set
test_X2_norm

#to balance imbalanced labels in ds with multilabel classes inside
from sklearn.utils import class_weight
sample_weights2 = class_weight.compute_sample_weight(class_weight="balanced", y=y2_train) # balancing sample weights for multilabel problem

"""tried with and tried without- didnt introduce any special change, decided to switch it off in the model code"""

#3d model with label upsampling
n_features=31 # input columns
n_classes=5 #classes

batch_size= 200
n_epochs= 50
verbosity=1
validation_split=0.15 #in case we split for train-test only 

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5) #goal is min loss-difference between predicted and actual
#the callback will stop the training when there is no improvement in the loss for 5 consecutive epochs

# create the third model

model3 = Sequential()
model3.call = tf.function(model3.call)
model3.add(Dense(62, activation='relu', input_dim=n_features)) #31*2
model3.add(Dropout(0.2)) #using dropout on hidden layers
model3.add(Dense(31, activation='relu'))
model3.add(Dropout(0.2))
model3.add(Dense(15, activation='relu'))
model3.add(Dropout(0.2))
model3.add(Dense(8, activation='relu')) #make the dense thinner
model3.add(Dropout(0.2))
model3.add(Dense(n_classes, activation='sigmoid')) #sigmoid in output to handle multilabel classification problems

# compile the model
model3.compile(loss=binary_crossentropy,
              optimizer=Adam(learning_rate=0.01), #tume learning rate=0.000001=the result got worse
              #metrics=['categorical_accuracy']
              #metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()]) #for binary_accuracy =0,89
              metrics = [tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()])
              #metrics=[tf.keras.metrics.Precision()])

model3.summary() #add the data about the model

# fit data to model
history = model3.fit(train_X2_norm, y2_train,
          #sample_weight=sample_weights2,
          batch_size=batch_size,
          epochs=n_epochs,
          verbose=verbosity,
          callbacks=[callback],
          validation_split=validation_split,
          shuffle=True)
          


# generate generalization metrics
score = model3.evaluate(test_X2_norm, y2_test, verbose=0)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}') #categorical 
#using binary metrics-the output is binary accuracy

"""Accuracy got improved-from 0,86 to 0,92 improving the training accuracy, increased in validation accuracy, validation auc increased,training loss decreased, val loss decreased (super small number)

val auc is high, auc of 1 is desirable
"""

#row = [10.24,	10.35, 4,	0.145,	0.00, 'walkingDistance',	'ptOnly',	'[travelTime, price]',	282.580,	226954,	259174,	'high',	0,	12,	220,	'achieved',	154.75,	0.60]

#to make random predictions based on artifical row with the data,31 features=31
row = [5.24,	10.35,	4,	0.153,	0.00,	238.651,	415543,	259174,	0,	1,	220,	51.12,	0.400,	0,	1,	0,	0,	0,	0,	1,	0, 0,	1,	0,	0,	1,	0,	0,	1,	0,	0]
from numpy import asarray
newX = asarray([row])
yhat = model3.predict(newX)
print('Predicted: %s' % yhat[0])

"""based on array(['blablacar', 'car', 'db_fv', 'flight', 'flixbus'], dtype=object) predicted that ['db_fv', 'flight', 'flixbus'] will be involved in trip"""

row1 = [2.24,	2.35,	1,	0.01,	0.20,	138.651,	115543,	359174,	1,	0,	30,	161.12,	0.600,	1,	0,	1,	0,	0,	0,	1,	0, 0,	1,	0,	0,	1,	0,	0,	1,	0,	0]
newX1 = asarray([row1])
yhat1 = model3.predict(newX1)
print('Predicted: %s' % yhat1[0])

"""based on array(['blablacar', 'car', 'db_fv', 'flight', 'flixbus'], dtype=object)
predicted that ['db_fv','flixbus'] will be involved in trip as modes
"""

from sklearn.metrics import classification_report #check the classification report
pred = model3.predict(test_X2_norm, batch_size=200, verbose=1)
predicted = np.argmax(pred, axis=1)
report = classification_report(np.argmax(y2_test, axis=1), predicted)#true target values of data +predicted output of model
print(report)

"""Lower support for flight in test data-again indicators are 0s everywhere-only 1 record

Indicators precision, recall and f-1 score for all the labels:

Precision- we got an extremely high precision of 1 for the 3 classes (blablacar,car,db_fv) (correctly predicted positive to the total positive)=>low false positive rate there, for flixbus it is higher than 0.5

However the 3d index again-flight- we as before were always getting 0 for precision and other indicators , also probably by the reason of low support (only 1 value in testing data) the 5th label flixbus got decreased in comp with previous approach

Recall(sensitivity): high everywhere except 4th flight

F1-score:F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost( could be in our case, however the client would like to be in a situation of false positive (flight was predicted, but actually we do not have it- in order to foresee the risks anyway) than to have false negative (actually we have a flight but it was predicted that it shouldnt be and a customer would undercalculate his risks) -high everywhere apart from 3d class-0 

it was smote technique implemented labelwise and the result is the most balanced one (however no normal involvement of flight with support)

Support is the number of actual occurrences of the class in the specified dataset. We balanced support in the training data but in testing data the support is still low
"""

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix
y_pred2 = model3.predict(test_X2_norm)
confusion_matrix2 = sklearn.metrics.multilabel_confusion_matrix(y2_test, np.rint(y_pred2))
confusion_matrix2

"""decreased even more in false negatives, which is a good sign"""

pip install shap #feature importance technique which worked in this case

import tensorflow.compat.v1.keras.backend as K
import tensorflow as tf
tf.compat.v1.disable_eager_execution() # to allow shap working based on graph mode

pip install eli5 #feature importance tried another one

from keras.wrappers.scikit_learn import KerasClassifier,KerasRegressor
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(model3, random_state=1,scoring="accuracy").fit(X2_train,y2_train)
eli5.show_weights(perm, feature_names = X2_train.columns.tolist())
#is not applicable for multilabel problems

pip install lime #feature importance tried 3d one

import lime.lime_tabular

def prob(data):
    print(data.shape)
    y_pred=model3.predict(data).reshape(-1, 1)
    y_pred =(y_pred>0.5)
    print(np.array(list(zip(1-y_pred.reshape(data.shape[0]),y_pred.reshape(data.shape[0])))))
    return np.hstack((1-y_pred,y_pred))
explainer = lime.lime_tabular.LimeTabularExplainer(X2_train[list(X2_train.columns)].astype(int).values,  
mode='classification',training_labels=parse['modelabel_final'],feature_names=list(X2_train.columns))
#didnt work with the case- 'Deep' object has no attribute 'explain_instance'

import shap #SHapley Additive exPlanations
import tensorflow.keras.backend 
tf.compat.v1.disable_eager_execution()
shap.initjs() #helped to run the shap
background = train_X2_norm[np.random.choice(train_X2_norm.shape[0], 100, replace = False)]# we use the first 100 training examples as our background dataset to integrate over
explainer = shap.DeepExplainer(model3, background) #Usage of the DeepExplainer for Keras, also could be applicable GradientExplainer, in some cases KernelExplainer

X2_train_summary = shap.sample(train_X2_norm, 1000) #based on sampled data since the whole trainig set is big and could affect running time and RAM
#X2_train_summary = shap.kmeans(train_X2_norm, 200) didnt work-'DenseData' object has no attribute 'shape'

shap_values = explainer.shap_values(X2_train_summary) #based on previous sample

shap.summary_plot(shap_values,X2_test,plot_type="bar")

"""Evaluate on 2d and 3 modelling with smote, since we didnt implement 'call' for the first model before the compiling

top 3 important features- delay_probability ( super high correlation with target), safety_boost and considered preferences with all preferences included.
The reason could be hidden in high dependencies between these features and target variable. 
delay feature mostly pushed the predictions of 4th class flixbus and least of flight-class 3(4th label), 'safety boost' pushed to 0 blablacar label
"""

#try on the 2d case with smote for the data imbalances
background2 = train_X1_norm[np.random.choice(train_X1_norm.shape[0], 100, replace = False)]# we use the first 100 training examples as our background dataset to integrate over
explainer2 = shap.DeepExplainer(model1, background2)
#train_X1_norm

X1_train_summary = shap.sample(train_X1_norm, 1000)#utilize smaller sample

shap_values1 = explainer.shap_values(X1_train_summary)

shap.summary_plot(shap_values1,X1_test,plot_type="bar") #Only plot_type = 'bar' is supported for multi-output explanations!

"""The pattern for the 3d model based on 1000 records as sample, safety boost is a bit more important on scale, balancing of classes pushed towards is the same
The 3d feature is 'multimodality' here
 The reason could be hidden in high correlation between these features and target variable. 'multimodaity' pushed to the 3d label- train
"""

from keras.models import load_model #use function
model1.save('trip_model.h5') #ability to save a model
model3.save('trip1_model.h5')

"""save our 2 models"""

new_model=load_model('trip_model.h5')#create new model

new_model.get_weights()# same as in previous model2

"""check the weights that were assigned"""

new_model.optimizer #check adam

"""Other approaches to deal with multilabel problem

references
https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff

Quick exercise for Appendix purposes to asses other techniques to deal with multilabel problem and the chances to work with our preprocessed data

http://scikit.ml/modelselection.html

https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/
"""

pip install scikit-multilearn #load the library

"""Most traditional learning algorithms are developed for single-label classification problems. Therefore a lot of approaches in the literature transform the multi-label problem into multiple single-label problems, so that the existing single-label algorithms can be used.

Basically these are the problem transformation methods:
Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers.

However we would keep in mind that the data prepared for NN (with all the features and leaving the extreme outliers would be inserted inside only for curiosity purpose), normally we would need to remove the outliers and we could also try label encoding for some categorical records (used one hot since its the most suitable option for NN)
"""

# using Label Powerset
from skmultilearn.problem_transform import LabelPowerset
from sklearn.naive_bayes import GaussianNB

# initialize Label Powerset multi-label classifier
# with a gaussian naive bayes base classifier
classifier = LabelPowerset(GaussianNB())

# train
classifier.fit(train_X_norm, y_train)

# predict
predictions = classifier.predict(test_X_norm)

accuracy_score(y_test,predictions)

"""almost 1 as the accuracy for the first portion of data prepared

Does take possible correlations between class labels into account. More commonly this approach is called the label-powerset method, because it considers each member of the power set of labels in the training set as a single label.
This method needs worst case (2^|C|) classifiers, and has a high computational complexity.
However when the number of classes increases the number of distinct label combinations can grow exponentially. This easily leads to combinatorial explosion and thus computational infeasibility. Furthermore, some label combinations will have very few positive examples.
"""

# using Label Powerset
from skmultilearn.problem_transform import LabelPowerset
from sklearn.naive_bayes import GaussianNB

# initialize Label Powerset multi-label classifier
# with a gaussian naive bayes base classifier
classifier = LabelPowerset(GaussianNB())

# train
classifier.fit(train_X1_norm, y1_train)

# predict
predictions = classifier.predict(test_X1_norm)

accuracy_score(y1_test,predictions)

"""almost the same ratio for all data"""

# using Label Powerset
from skmultilearn.problem_transform import LabelPowerset
# initialize label powerset multi-label classifier
from sklearn.linear_model import LogisticRegression
classifier = LabelPowerset(LogisticRegression()) #worse with Logistics regression
# train
classifier.fit(train_X_norm, y_train)
# predict
predictions = classifier.predict(test_X_norm)
# accuracy
print("Accuracy = ",accuracy_score(y_test,predictions))
print("\n")

# using Label Powerset
from skmultilearn.problem_transform import LabelPowerset
# initialize label powerset multi-label classifier
from sklearn.linear_model import LogisticRegression
classifier = LabelPowerset(LogisticRegression()) #worse with Logistics regression
# train
classifier.fit(train_X1_norm, y1_train)
# predict
predictions = classifier.predict(test_X1_norm)
# accuracy
print("Accuracy = ",accuracy_score(y1_test,predictions))
print("\n")

# using binary relevance
from skmultilearn.problem_transform import BinaryRelevance
from sklearn.naive_bayes import GaussianNB

# initialize binary relevance multi-label classifier
# with a gaussian naive bayes base classifier
classifier = BinaryRelevance(GaussianNB())

# train
classifier.fit(train_X_norm, y_train)

# predict
predictions = classifier.predict(test_X_norm)
accuracy_score(y_test,predictions)

"""poor level of accuracy

In this case an ensemble of single-label binary classifiers is trained, one for each class. Each classifier predicts either the membership or the non-membership of one class. The union of all classes that were predicted is taken as the multi-label output. This approach is popular because it is easy to implement, however it also ignores the possible correlations between class labels.
In other words, if there’s q labels, the binary relevance method creates q new data sets from the data, one for each label and train single-label classifiers on each new data set. One classifier may answer yes/no to the question “does it contain train?”, thus the “binary” in “binary relevance”.
"""

# using binary relevance
from skmultilearn.problem_transform import BinaryRelevance
from sklearn.naive_bayes import GaussianNB

# initialize binary relevance multi-label classifier
# with a gaussian naive bayes base classifier
classifier = BinaryRelevance(GaussianNB())

# train
classifier.fit(train_X2_norm, y2_train)

# predict
predictions = classifier.predict(test_X2_norm)
accuracy_score(y2_test,predictions)

"""the accuracy is almost 2 times lower for the second portion of data with smote pes samples in compariosn to th first one"""

# using classifier chains
from skmultilearn.problem_transform import ClassifierChain
from sklearn.naive_bayes import GaussianNB

# initialize classifier chains multi-label classifier
# with a gaussian naive bayes base classifier
classifier = ClassifierChain(GaussianNB())

# train
classifier.fit(train_X1_norm, y1_train)

# predict
predictions = classifier.predict(test_X1_norm)

accuracy_score(y1_test,predictions)

"""A chain of binary classifiers C0, C1, . . . , Cn is constructed, where a classifier Ci uses the predictions of all the classifier Cj , where j < i. This way the method, also called classifier chains (CC), can take into account label correlations.
The total number of classifiers needed for this approach is equal to the number of classes, but the training of the classifiers is more involved.
"""

# using classifier chains
from skmultilearn.problem_transform import ClassifierChain
from sklearn.naive_bayes import GaussianNB

# initialize classifier chains multi-label classifier
# with a gaussian naive bayes base classifier
classifier = ClassifierChain(GaussianNB())

# train
classifier.fit(train_X_norm, y_train)

# predict
predictions = classifier.predict(test_X_norm)

accuracy_score(y_test,predictions)

# using classifier chains
from skmultilearn.problem_transform import ClassifierChain
from sklearn.linear_model import LogisticRegression #with Logistic Regression
# initialize classifier chains multi-label classifier
classifier = ClassifierChain(LogisticRegression())
# Training logistic regression model on train data
classifier.fit(train_X_norm, y_train)
# predict
predictions = classifier.predict(test_X_norm)
# accuracy
print("Accuracy = ",accuracy_score(y_test,predictions))
print("\n")

"""the highest accuracy in combination with LogReg"""

# using classifier chains
from skmultilearn.problem_transform import ClassifierChain
from sklearn.linear_model import LogisticRegression #with Logistic Regression
# initialize classifier chains multi-label classifier
classifier = ClassifierChain(LogisticRegression())
# Training logistic regression model on train data
classifier.fit(train_X1_norm, y1_train)
# predict
predictions = classifier.predict(test_X1_norm)
# accuracy
print("Accuracy = ",accuracy_score(y1_test,predictions))
print("\n")