# -*- coding: utf-8 -*-
"""extension- K-prototypes modified labels 4 clusters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bd_rDyXeMzkwjc8gyZFEXJjvIP6noaJy
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
#to include graphs next to the code
#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='withoutcolumns.csv'
df = pd.read_csv(file)
print(df.describe()) #statistics

#redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)
df.drop('index', axis=1, inplace=True)

#use proper time units
df.rename({'totaltraveltimeinsec': 'totaltraveltimeinhours'}, axis=1, inplace=True)
df

#check if we have appropriate values
column = df["totalwalkingdistance"]
max_value = column.max()
max_value

df=df.round({'totalwalkingdistance': 3})
df

#we do not have missing values, because we use the previous approach with 0s for them for the trial clustering round
df.isna().sum()

#make a copy of data to perform Hopkins Statistics to decide on whether our data could be clustered or not
#involved numerical columns
#will repeat it after encoding of categorical for DBscan and check what was the coeficient
dfhopkins=df.copy()

#drop categorical columns for the statistics
dfhopkins.drop(dfhopkins.columns[[5,6,7,8,9,10]], axis = 1, inplace = True)

#check the newest dataframe- only numerical are inside
dfhopkins

import sklearn 
from sklearn.preprocessing import StandardScaler
# we need to standardize values through scaler
scaler = StandardScaler()
# fit_transform
dfnew_scaled = scaler.fit_transform(dfhopkins)

#checck the data
dfnew_scaled

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan
 
def hopkins(X):
    d = X.shape[1]
    #d = len(vars) # columns
    n = len(X) # rows
    m = int(0.1 * n) # heuristic from article [1]
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)
 
#Generate a simulated data set (random D) drawn from a random uniform distribution with n points (q1,q2,…, qn) and the same variation as the original real data set D.
    rand_X = sample(range(0, n, 1), m)
 #Compute the distance, xi, from each real point to each nearest neighbour: For each point, pi ∈ D, find it’s nearest neighbour pj; 
 #then compute the distance between pi and pj and denote it as xi=dist(pi,pj)
    ujd = []
#Compute the distance, yi from each artificial point to the nearest real data point: For each point qi ∈ random D, find it’s nearest neighbour qj in D; then compute the distance between qi and qj and denote it yi=dist(qi,qj)
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])
#Calculate the Hopkins statistic (H) as the mean nearest neighbour distance in the random data set divided by the sum of the mean nearest neighbour distances in the real and across the simulated data set.
    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0
 
    return H

#convert “dfscaled” to pandas DataFrame & define previouscolumn names 
dfnew_scaled = pd.DataFrame(dfnew_scaled)
dfnew_scaled.columns =['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime']

#apply Hopkins function to the dataset
#the clusters should be meaningful considering the numerical columns
#function hopkins compare scatter spread of data points from “Iris_scaled” to Random scatter data which contains no cluster tendency or properties,using NearestNeighbors
#the test tells us how much percentage different is our data from random scatter data
# can do Hopkins test before scaling or after the scaling as the scale of x-axis & y-axis changes it does not affect the spread of points

hopkins(dfnew_scaled)

#can conduct the Hopkins Statistic test iteratively, using 0.5 as the threshold to reject the alternative hypothesis.
#if H < 0.5, then it is unlikely that D has statistically significant clusters.
#if the value of Hopkins statistic is close to 1, then we can reject the null hypothesis and conclude that the dataset D is significantly a clusterable data.
#our values is extremely close to 0

#define what columns we need to drop for the clustering
# goal is to cluster the group of trips and see maybe we can detect some new patterns in the data based on clusters
#was decided to drop column objective- doesnt have any meaningfull information for clustering
#we have completely identical rows with all numerical/categorical variables (having the same labesls) with difference only in objective=> it doesnt change anything
#it also repeats somehow the combinations inside the 'considered preferences" column, whoch we will use for clustering

#drop sourcename and targetname- we would need to consider the groups of trips in Germany in general


df.drop(df.columns[[5, 9, 10]], axis = 1, inplace = True)

#the duplicated were removed
df.drop_duplicates(keep='last', inplace=True)

#from 85153 to 30199 thousands of rows (mainly because of re ducing the objective column)
df

#dropping the outliers of price using IQR method-interquartile range 
#statistically, it is assumesd that the values are clustered around some central value, i.e. IQR = Q3 – Q1 
#if a data point is below Q1 – 1.5×IQR or above Q3 + 1.5×IQR, it is considered as being too far from the central values

q1 = df['totalprice'].quantile(0.25)
q3 = df['totalprice'].quantile(0.75)
iqr = q3 - q1     

#some extreme data values have been removed
filter = (df['totalprice'] >= (q1 - 1.5 * iqr)) & (df['totalprice'] <= (q3 + 1.5 *iqr))
df1=df.loc[filter]#filtering the data

df1.head(10)

df11

#dropping the outliers of total travel time using IQR method-interquartile range 
#statistically, it is assumesd that the values are clustered around some central value, i.e. IQR = Q3 – Q1 
#if a data point is below Q1 – 1.5×IQR or above Q3 + 1.5×IQR, it is considered as being too far from the central values

q1 = df1['totaltraveltimeinhours'].quantile(0.25)
q3 = df1['totaltraveltimeinhours'].quantile(0.75)
iqr = q3 - q1     

#some extreme data values have been removed
filter = (df1['totaltraveltimeinhours'] >= (q1 - 1.5 * iqr)) & (df1['totaltraveltimeinhours'] <= (q3 + 1.5 *iqr))
df11=df1.loc[filter]#filtering the data

df11.head(10)

#removed extreme outliers for tine and price
plt.figure(figsize=(15,10))
sns.boxplot(x='totalnumberofchanges',y='totaltraveltimeinhours',data=df11.sort_values('totaltraveltimeinhours',ascending=False))

# after dropping the columns we still have all set of unique values inside regarding the categorical set of values per column
for c in df:
    print(df[c].unique())
    print(df[c].nunique())



#33 unique combinations for the column "labels",make it more general with the transport involved in itinerary to report which combinations where used
#experimentl solution to report which modes in general are applicable to the cluster

df11['finalsolutionusedlabels'] = df11['finalsolutionusedlabels'].replace(['[db_fv, blablacar]','[db_fv, blablacar, db_fv]'],'[blablacar, db_fv]')
df11['finalsolutionusedlabels'] = df11['finalsolutionusedlabels'].replace(['[db_fv, flixbus]','[db_fv, flixbus, db_fv]','[db_fv, flixbus, db_fv, flixbus]','[db_fv, flixbus, db_fv, flixbus, db_fv]','[flixbus, db_fv, flixbus, db_fv]','[flixbus, db_fv, flixbus]'],'[flixbus, db_fv]')
df11['finalsolutionusedlabels'] = df11['finalsolutionusedlabels'].replace(['[flixbus, blablacar, db_fv]','[db_fv, blablacar, flixbus]','[db_fv, blablacar, flixbus, db_fv]','[blablacar, db_fv, flixbus]','[blablacar, flixbus, db_fv]','[db_fv, flixbus, blablacar, flixbus]','[flixbus, blablacar, db_fv, flixbus]','[flixbus, db_fv, blablacar]','[flixbus, blablacar, flixbus, db_fv]','[blablacar, db_fv, flixbus, db_fv]','[db_fv, blablacar, db_fv, flixbus]'],'[db_fv, flixbus, blablacar]')
df11['finalsolutionusedlabels'] = df11['finalsolutionusedlabels'].replace(['[flixbus, blablacar]','[flixbus, blablacar, flixbus]'],'[blablacar, flixbus]')
df11['finalsolutionusedlabels'] = df11['finalsolutionusedlabels'].replace(['[flixbus, flight, db_fv, flixbus]','[db_fv, flight, flixbus, db_fv]'],'[flixbus, flight, db_fv]')

#from 33 to 10 transport solutions
uniqueValues1 = df11['finalsolutionusedlabels'].unique()
uniqueValues1

df.info()

#copy of current dataframe
X = df11.copy()

from sklearn.preprocessing import StandardScaler
scaled_X = StandardScaler().fit_transform(X[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime']])
X[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime']] = scaled_X

pip install kmodes

from kmodes.kprototypes import KPrototypes
#dataframe to an array because this is a requirement for kprototypes algorithm
new_array = X.values
#converting numerical columns datatype as float for kprototype algorithm
new_array[:, 0] = new_array[:,0].astype(float)
new_array[:, 1] = new_array[:,1].astype(float)
new_array[:, 2] = new_array[:,2].astype(float)
new_array[:, 3] = new_array[:,3].astype(float)
new_array[:, 4] = new_array[:,4].astype(float)

#index of categorical columns- would be 5,6,7
categorical_index = list(range(5,8))

# Function for plotting elbow curve
def plot_elbow_curve(start, end, data):
  no_of_clusters = list(range(start, end+1))
  cost_values = []
    
  for k in no_of_clusters:
      test_model = KPrototypes(n_clusters=k, init='Huang', random_state=42)
      test_model.fit_predict(data, categorical=categorical_index)
      cost_values.append(test_model.cost_)
        
  sns.set_theme(style="whitegrid", palette="bright", font_scale=1.2)
    
  plt.figure(figsize=(15, 7))
  ax = sns.lineplot(x=no_of_clusters, y=cost_values, marker="o", dashes=False)
  ax.set_title('Elbow curve', fontsize=18)
  ax.set_xlabel('No of clusters', fontsize=14)
  ax.set_ylabel('Cost', fontsize=14)
  ax.set(xlim=(start-0.1, end+0.1))
  plt.plot();
    
# Plotting elbow curve for k=2 to k=10
plot_elbow_curve(2,10,new_array)

#we will try out 4 clusters for this case

cl_4 = KPrototypes(n_clusters=4, init='Huang', random_state=42, n_jobs=-1)
cl_4.fit_predict(new_array, categorical=categorical_index)
print(cl_4.cost_)

#new column for cluster labels associated with each subject
X['labels'] = cl_4.labels_

df11['K-proto cluster']=cl_4.labels_
df11

X

#conclusion on clusters
pd.set_option('max_rows',None)
df11['labels'] = cl_4.labels_
df11.groupby('labels').agg(['median' ,'mean']).T

#again check the results for 4 cluster
cluster_1= df11[df11['labels']== 0]
cluster_1.head(50) #get the results

#again check the results for 4 cluster
cluster_2= df11[df11['labels']== 1]
cluster_2.head(50) #get the results

#again check the results for 4 cluster
cluster_3= df11[df11['labels']== 2]
cluster_3.head(50) #get the results

#again check the results for 4 cluster
cluster_4= df11[df11['labels']== 3]
cluster_4.head(50) #get the results

#plot the data to reveal the mode types involved

def plot0(datt):
  plt.figure(figsize=(15, 10))
  sns.boxplot(data=datt,x="finalsolutionusedlabels", y='totalprice',palette='Set3')#take care about variables of the data
  plt.title('Price distribution per clustered groups')
  plt.xticks(rotation='vertical')
  plt.xlabel('Transport')
  plt.ylabel("Price value")


plot0(cluster_1)
plot0(cluster_2)
plot0(cluster_3)
plot0(cluster_4)

def plot6(data):
# top proposed solutions
  cnt_srs = data['finalsolutionusedlabels'].value_counts().nlargest(3)
  plt.figure(figsize=(15,10))
  sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="Blues_r")
  plt.title('Top proposed transportation solutions per clustered group')
  plt.xticks(rotation='vertical')
  plt.xlabel('Final solution of the algorithm per cluster', fontsize=12)
  plt.ylabel('Number of obseravtions', fontsize=12)
  plt.show()

plot6(cluster_1)
plot6(cluster_2)
plot6(cluster_3)
plot6(cluster_4)

