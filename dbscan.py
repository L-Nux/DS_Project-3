# -*- coding: utf-8 -*-
"""DBSCAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CVIy8PcB2fIqgoXMPEK9ZbJ5kxSlov4P
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
#to include graphs next to the code
#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='withoutcolumns.csv'
df = pd.read_csv(file)
print(df.describe()) #statistics

#redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)
df.drop('index', axis=1, inplace=True)

df.rename({'totaltraveltimeinsec': 'totaltraveltimeinhours'}, axis=1, inplace=True)
df

#we do not have missing values, because we use the previous approach with 0s for them for the trial clustering round
df.isna().sum()

#define what columns we need to drop for the clustering
# goal is to cluster the group of trips and see maybe we can detect some new patterns in the data based on clusters
#was decided to drop column objective-super low degree of association-0,05.. doesnt have any meaningfull information for clustering
#we have completely identical rows with all numerical/categorical variables (having the same labesls) with difference only in objective=> it doesnt change anything
#it also repeats somehow the combinations inside the 'considered preferences" column, whoch we will use for clustering

#drop sourcename and targetname for the algorithm- we would need to consider the groups of trips in Germany in general


df.drop(df.columns[[5, 9, 10]], axis = 1, inplace = True)

#the duplicated were removed
df.drop_duplicates(keep='last', inplace=True)

#from 85153 to 30199 thousands of rows (mainly because of reducing the objective column)
df

#try 2 combinations for dbscan- only involving numerical data and numerical + encoded categorical columns since encoding could distort the clusters
#use label encoding for the column considered preferences because it has the order

#use one hot encoding for finitiautomation and labels (or drop the labels)

encoding=df.copy()

from sklearn.preprocessing import LabelEncoder #encode the values
labelencoder = LabelEncoder()

#use label encoding for the column considered preferences because it has the order
encoding['consideredpreferences'] = labelencoder.fit_transform(encoding['consideredpreferences'])#Fit label encoder and return encoded labels for the first target column
#use label encoding because we have too many values for one hot encoding for solutionlabels-33 unique combinatios
encoding['finalsolutionusedlabels'] = labelencoder.fit_transform(encoding['finalsolutionusedlabels'])#Fit label encoder and return encoded labels for the second one

encoding.head(10)

#In the case of one-hot encoding, for N categories in a variable, it uses N binary variables. 
#The dummy encoding is a small improvement over one-hot-encoding, uses N-1 features to represent N labels/categories.
#references are here 
#https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/

#We apply One-Hot Encoding when:

#The categorical feature is not ordinal (like the countries above)
#The number of categorical features is less so one-hot encoding can be effectively applied
#We apply Label Encoding when:

#The categorical feature is ordinal (like Jr. kg, Sr. kg, Primary school, high school)
#The number of categories is quite large as one-hot encoding can lead to high memory consumption

#we got our transformed dataset to plug it into dbscan algorithm and check the clusters
#use all the features to cluster the observations
categorical_col= ["finiteautomaton"]
transformed = pd.get_dummies(encoding, columns=categorical_col)#convert categorical target our automation

#check what we have got as the result
transformed.head(10)

#create the second correlation matrix using our transformed categorical data
show=transformed.corr(method='kendall')#pairwise correlation
plt.figure(figsize=(10,10))
sns.heatmap(show,annot=True,cmap='coolwarm')

# reference:https://www.statisticssolutions.com/pearsons-correlation-coefficient/
# meaning of correlation number
# Perfect: If the value is near ± 1, then it said to be a perfect correlation
# High degree: If the coefficient value lies between ± 0.50 and ± 1, then it is said to be a strong correlation.
# Moderate degree: If the value lies between ± 0.30 and ± 0.49, then it is said to be a medium correlation.
# Low degree: When the value lies below ± 0.29, then it is said to be a small correlation.
# No correlation: When the value is zero


# no strong correlation between transformed categorical and numerical was detected

#import further libraries for DBscan
from sklearn.preprocessing import MinMaxScaler # for feature scaling
from sklearn import metrics # for calculating Silhouette score

import matplotlib.pyplot as plt # for data visualization
import plotly.graph_objects as go # for data visualization
import plotly.express as px # for data visualization

from sklearn.cluster import DBSCAN

# plotting in 3d scatter plot just with the observations before clustering
# we will assign the clusters there afterwards
fig = px.scatter_3d(transformed, x=transformed['totalprice'], y=transformed['totaltraveltimeinhours'], z=transformed['totalnumberofchanges'],
                 opacity=1, color_discrete_sequence=['green'], height=900, width=900)

# Update chart looks
fig.update_layout(#title_text="Scatter 3D Plot",
                  scene_camera=dict(up=dict(x=0, y=0, z=1), 
                                        center=dict(x=0, y=0, z=-0.2),
                                        eye=dict(x=1.5, y=1.5, z=0.5)),
                                        margin=dict(l=0, r=0, b=0, t=0),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='blue',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         ),
                               yaxis=dict(backgroundcolor='white',
                                          color='blue',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                          ),
                               zaxis=dict(backgroundcolor='lightgrey',
                                          color='blue', 
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         )))

# update marker size
fig.update_traces(marker=dict(size=2))

fig.show()

#to scale the features used min max scaler because data distribution is not normal (not fit a gaussian distribution)
scaler = MinMaxScaler()
transformed_scaled = scaler.fit_transform(transformed)

# plot the min max scaled distributions (dataset is combined)
#checked that all of them are between 1 and 0
fig, axs = plt.subplots(1, 5, figsize=(16,4), dpi=300)
axs[0].hist(transformed_scaled[:,0], bins=50, color='blue', rwidth=0.9)
axs[0].set_title('totaltraveltimeinhours')
axs[1].hist(transformed_scaled[:,1], bins=50, color='blue', rwidth=0.9)
axs[1].set_title('totalprice')
axs[2].hist(transformed_scaled[:,2], bins=50, color='green', rwidth=0.9)
axs[2].set_title('totalnumberofchanges')
axs[3].hist(transformed_scaled[:,3], bins=50, color='green', rwidth=0.9)
axs[3].set_title('totalwalkingdistance')
axs[4].hist(transformed_scaled[:,4], bins=50, color='green', rwidth=0.9)
axs[4].set_title('totalwaitingtime')

plt.show()

#check the array
transformed_scaled

#check the shape
transformed_scaled.shape

#test different hyperparameter values for DBscan
# empty lists
# to store inside different Silhouette scores
SS=[] # to store inside different Silhouette scores
# to save results of various epsilon and min_samples
combinations=[] 

# ranges to choose
# we will choose different suitables ranges to test
eps_range=range(6,13) # 0.6 - 1.3 range
#rule of thumb-choose based on number of dimensions-before scaling it was 7, now its 9, try out other options
minpts_range=range(7,12)

for k in eps_range:
    for j in minpts_range:
        # model and parameters
        model = DBSCAN(eps=k/10, min_samples=j)
        # fitting the model 
        clm = model.fit(transformed_scaled)
        # calculation of Silhoutte Score and appending it to the list
        SS.append(metrics.silhouette_score(transformed_scaled, clm.labels_, metric='euclidean'))
        combinations.append(str(k)+"|"+str(j)) # axis values for the graph

# plotting the results of Silhouette scores to make a decision
plt.figure(figsize=(20,8), dpi=300)
plt.plot(combinations, SS, 'bo-', color='black')
plt.xlabel('Epsilon/10 | MinSample Z')
plt.ylabel('Silhouette Coefficient')
plt.title('Silhouette Score with various combinations of hyperparameters')
plt.show()

#running time is 20 minutes
# we can conclude that epsilon 0,8 and 7 minpts produce the highest silhouette score, but still the value of 0,15 is really low

#eventually use for the model: eps=0.8, MinPts=7

modeldb = DBSCAN(eps=0.8, # default=0.5, The maximum distance between two samples for one to be considered as in the neighborhood of the other.
               min_samples=7, # default=5, The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.
               metric='euclidean', # default='euclidean'. The metric to use when calculating distance between instances in a feature array. 
               metric_params=None, # default=None, Additional keyword arguments for the metric function.
               algorithm='auto', # {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’, The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors.
               leaf_size=30, # default=30, Leaf size passed to BallTree or cKDTree.
               p=None, # default=None, The power of the Minkowski metric to be used to calculate distance between points. If None, then p=2
               n_jobs=None, # default=None, The number of parallel jobs to run. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.
              )



# fit our model
clustm = modeldb.fit(transformed_scaled)


# check the results
print('DBSCAN Clustering Model ')
print("Cluster labels for the first model")
print(clustm.labels_)

#3 clusters were formed, having no -1 means that we have no noise being identified by the algorithm

#assign clusters to the data frame for further analysis of groups via plots
df['DBSCAN cluster']=clustm.labels_
df

# plot the clusters in 3d environment with the colors assigned per cluster
df=df.sort_values(by=['DBSCAN cluster'])

# 3d scatter plot
fig = px.scatter_3d(df, x=df['totalprice'], y=df['totaltraveltimeinhours'], z=df['totalnumberofchanges'], 
                    opacity=1, color=df['DBSCAN cluster'].astype(str), 
                    color_discrete_sequence=['black']+px.colors.qualitative.Plotly,
                    hover_data=['totalwalkingdistance', 'totalwaitingtime'],
                    width=900, height=900
                   )

# chart
fig.update_layout(#title_text="Scatter 3D Plot",
                  showlegend=True,
                  legend=dict(orientation="h", yanchor="bottom", y=0.04, xanchor="left", x=0.1),
                  scene_camera=dict(up=dict(x=0, y=0, z=1), 
                                        center=dict(x=0, y=0, z=-0.2),
                                        eye=dict(x=1.5, y=1.5, z=0.5)),
                                        margin=dict(l=0, r=0, b=0, t=0),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         ),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                          ),
                               zaxis=dict(backgroundcolor='lightgrey',
                                          color='black', 
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         )))
# change marker size
fig.update_traces(marker=dict(size=2))

fig.show()

#with the silhouette 0,15 we can conclude that clusters are indifferent since the distance between them is not really significant

#check how many instances belong to the cluster
print(pd.Series(clustm.labels_).value_counts())

df['DBSCAN cluster']=clustm.labels_

#0,1,2 our 3 groups
clustm.labels_

#to plot 2 dimensions
dbscan=DBSCAN(eps=0.8, min_samples=7)

lables=dbscan.fit_predict(transformed_scaled)

#2 dimensions
lables=dbscan.fit_predict(transformed_scaled)
plt.figure(figsize=(10, 10))
plt.scatter(transformed_scaled[lables == 0, 0], transformed_scaled[lables == 0, 1], s = 50, c = 'black')
plt.scatter(transformed_scaled[lables == 1, 0], transformed_scaled[lables == 1, 1], s = 50, c = 'violet')
plt.scatter(transformed_scaled[lables == 2, 0], transformed_scaled[lables == 2, 1], s = 50, c = 'red')

plt.xlabel('Travel time')
plt.ylabel('Price')
plt.title('DBSCAN 3 Clusters of data')
plt.show()

#three unique clusters
np.unique(lables)

#form the group for cluster 1
cluster_1= df[df['DBSCAN cluster']== 0]
cluster_1.head(50) #get the results

#form the group for cluster 2
cluster_2= df[df['DBSCAN cluster']== 1]
cluster_2.head(50) #get the results

#form the group for cluster 3
cluster_3= df[df['DBSCAN cluster']== 2]
cluster_3.head(50) #get the results

def minvalue(data):
  minvalue = data[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice']].min()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice': ")
  print(minvalue)

minvalue(cluster_1)
minvalue(cluster_2)
minvalue(cluster_3)

#investigate the group with respect to max value
def maxvalue(d):
  maxvalue = d[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice']].max()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice': ")
  print(maxvalue)

maxvalue(cluster_1)
maxvalue(cluster_2)
maxvalue(cluster_3)


#the range would be from 1 hour to 47 hours,0-36 waiting hours, 0-5 changes,0-0.745 walking distance, 0.72-50.840 for the price

#investigate the group with respect to mean value
#no distinctive mean
def meanvalue(dat):
  meanvalue = dat[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice']].mean()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice': ")
  print(meanvalue)

meanvalue(cluster_1)
meanvalue(cluster_2)
meanvalue(cluster_3)


#values in general

#after dbscan got the groups per mode of transportation
#1st group is about all types of transport
#2d group is public only
#3d group without any flights
def plot1(data):
  plt.figure(figsize=(10,10))#check the size
  sns.kdeplot(x=data.totalnumberofchanges,y=data.totalwaitingtime,hue=data.finiteautomaton,shade=True)
  plt.title('Density estimation per clustered groups')
  plt.xlabel('Transfers')
  plt.ylabel("Waiting time")

plot1(cluster_1)
plot1(cluster_2)
plot1(cluster_3)

def plot6(data):
# top proposed solutions-normalyy the 1st is train, next the ratio is changeable
  cnt_srs = data['finalsolutionusedlabels'].value_counts().nlargest(5)
  plt.figure(figsize=(15,10))
  sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="Greens_r")
  plt.title('Top proposed transportation solutions per clustered group')
  plt.xticks(rotation='vertical')
  plt.xlabel('Final solution of the algorithm per cluster', fontsize=12)
  plt.ylabel('Number of obseravtions', fontsize=12)
  plt.show()

plot6(cluster_1)
plot6(cluster_2)
plot6(cluster_3)