# -*- coding: utf-8 -*-
"""K-prototypes final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DBXQdDGnX18fbsFwzl_ipRNbDneWCRLA
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
#to include graphs next to the code
#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='withoutcolumns.csv'
df = pd.read_csv(file)
print(df.describe()) #statistics

#redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)
df.drop('index', axis=1, inplace=True)

df.rename({'totaltraveltimeinsec': 'totaltraveltimeinhours'}, axis=1, inplace=True)
df

#we do not have missing values, because we use the previous approach with 0s for them for the trial clustering round
df.isna().sum()

#make a copy of data to perform Hopkins Statistics to decide on whether our data could be clustered or not
#involved numerical columns
#will repeat it after encoding of categorical for DBscan and check what was the coeficient
dfhopkins=df.copy()

#drop categorical columns for the statistics
dfhopkins.drop(dfhopkins.columns[[5,6,7,8,9,10]], axis = 1, inplace = True)

#check the newest dataframe- only numerical are inside
dfhopkins

import sklearn 
from sklearn.preprocessing import StandardScaler
# we need to standardize values through scaler
scaler = StandardScaler()
# fit_transform
dfnew_scaled = scaler.fit_transform(dfhopkins)

#check the data
dfnew_scaled

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan
 
def hopkins(X):
    d = X.shape[1]
    #d = len(vars) # columns
    n = len(X) # rows
    m = int(0.1 * n) # heuristic from article [1]
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)
 
#Generate a simulated data set (random D) drawn from a random uniform distribution with n points (q1,q2,…, qn) and the same variation as the original real data set D.
    rand_X = sample(range(0, n, 1), m)
 #Compute the distance, xi, from each real point to each nearest neighbour: For each point, pi ∈ D, find it’s nearest neighbour pj; 
 #then compute the distance between pi and pj and denote it as xi=dist(pi,pj)
    ujd = []
#Compute the distance, yi from each artificial point to the nearest real data point: For each point qi ∈ random D, find it’s nearest neighbour qj in D; then compute the distance between qi and qj and denote it yi=dist(qi,qj)
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])
#Calculate the Hopkins statistic (H) as the mean nearest neighbour distance in the random data set divided by the sum of the mean nearest neighbour distances in the real and across the simulated data set.
    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0
 
    return H

#convert “dfscaled” to pandas DataFrame & define previouscolumn names 
dfnew_scaled = pd.DataFrame(dfnew_scaled)
dfnew_scaled.columns =['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime']

#apply Hopkins function to the dataset
#the clusters should be meaningful considering the numerical columns
#function hopkins compare scatter spread of data points from our dataset to random scatter data which contains no cluster tendency or properties,using NearestNeighbors
#the test tells us how much percentage different is our data from random scatter data
# can do Hopkins test before scaling or after the scaling as the scale of x-axis & y-axis changes it does not affect the spread of points

hopkins(dfnew_scaled)

#can conduct the Hopkins Statistic test iteratively, using 0.5 as the threshold to reject the alternative hypothesis.
#if H < 0.5, then it is unlikely that D has statistically significant clusters.
#if the value of Hopkins statistic is close to 1, then we can reject the null hypothesis and conclude that the dataset D is significantly a clusterable data.
#our values is extremely close to 1

#define what columns we need to drop for the clustering
# goal is to cluster the group of trips and see maybe we can detect some new patterns in the data based on clusters
#was decided to drop column objective-super low degree of association-0,05.. doesnt have any meaningfull information for clustering
#we have completely identical rows with all numerical/categorical variables (having the same labesls) with difference only in objective=> it doesnt change anything
#it also repeats somehow the combinations inside the 'considered preferences" column, whoch we will use for clustering

#drop sourcename and targetname for the algorithm- we would need to consider the groups of trips in Germany in general


df.drop(df.columns[[5, 9, 10]], axis = 1, inplace = True)

#the duplicated were removed
df.drop_duplicates(keep='last', inplace=True)

#from 85153 to 30199 thousands of rows (mainly because of reducing the objective column)
df

#for k.prototypes we need float as the input
df.info()

#our numerical columns are within the different rnge we need to scale them for clustering
clscaled=df.copy()

#standardize features by removing the mean and scaling to unit variance
from sklearn.preprocessing import StandardScaler
scaled_new = StandardScaler().fit_transform(clscaled[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime']])
clscaled[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime']] = scaled_new

#check if everything got scaled
clscaled

# Get the position of categorical columns
#use preference, automation and transport
catColumnsPos = [df.columns.get_loc(col) for col in list(clscaled.select_dtypes('object').columns)]
print('Categorical columns           : {}'.format(list(clscaled.select_dtypes('object').columns)))
print('Categorical columns position  : {}'.format(catColumnsPos))

# Convert dataframe to matrix- we need it for kprototypes input as well as floats
dfmatrix = clscaled.to_numpy()
dfmatrix

#install the library
!pip install kmodes

#to check how many clusters?
from tqdm import tqdm
#running time is 1+hour
costs = []
n_clusters = []
clusters_assigned = []

cat_cols = [5,6,7]

for i in tqdm(range(2, 10)):
    try:
        kproto = KPrototypes(n_clusters=i, init='Huang', verbose=2)
        clusters = kproto.fit_predict(dfmatrix, categorical=cat_cols)
        costs.append(kproto.cost_)
        n_clusters.append(i)
        clusters_assigned.append(clusters)
    except:
        print(f"Can't cluster with {i} clusters")

#plot the curve
from plotly import graph_objects as go
fig = go.Figure(data=go.Scatter(x=n_clusters, y=costs))
fig.update_layout(
    title="Elbow method",
    xaxis_title="Clusters",
    yaxis_title="Costs",
    font=dict(
        family="Courier New, monospace",
        size=18,
        color="#7f7f7f"
    )
)
fig.show()

#obtained the result that the best number for clusters is 5 (also could be 4, but we have 5 distinct types of transport)
from kmodes.kprototypes import KPrototypes
from matplotlib import style
style.use("ggplot")
colors = ['b', 'g', 'r','y','m']

cat_cols = [5,6,7]
kproto = KPrototypes(n_clusters=5, init='Huang', verbose=2)
clusters = kproto.fit_predict(dfmatrix, categorical=cat_cols)

# Print cluster centroids of the trained model.
print(kproto.cluster_centroids_)

# Print training statistics
print(kproto.cost_)
print(kproto.n_iter_)

#check how many observations we have in each cluster
#2d cluster (here is 1) has the majority, 3d cluster(2) has the minority
print(pd.Series(clusters).value_counts())

#attach clusters to original dataframe df to see what separate groups we are having now
cluster_dict=[]
for c in clusters:
  cluster_dict.append(c)

#get all the values of clusters proposed with regards to individual records
cluster_dict

# go to the initial dataframe df and assign number of clusters
#copy df for 5 clusters
df5=df.copy()

df5['cluster']=cluster_dict

#to see if we really have the column clusters
df5

#get the fisrt 15 values for cluster 0, to understand a rough trend, relatively moderate travel time and high total price
#how to visualize to explain?
df5[df5['cluster']==0].head (20)

# define and map colors to visulize the clusters

colors = ['b', 'g', 'r', 'y', 'darkorange']
df5['c'] = df5.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3], 4:colors[4]})

#get the cluster centroids again
print(kproto.cluster_centroids_)

#convert “dfscaled” to pandas DataFrame & define previous column names 
centr = pd.DataFrame(kproto.cluster_centroids_)
centr.columns =['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime','finiteautomaton',	'consideredpreferences',	'finalsolutionusedlabels']

#append column cluster to the centroids space
s = pd.Series(['1','2', '3','4','5'], index=[0, 1, 2, 3, 4])
centr['cluster'] = s.values

centr

# define and map colors to visulize the clusters

colors = ['b', 'g', 'r', 'y', 'darkorange']
df5['c'] = df5.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3], 4:colors[4]})

#we can conclude that the clusters were perfectly defined
#the data got clustered in the distinguishable way 
#just plot per two main dimensions

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 10))
plt.scatter(df5.totaltraveltimeinhours, df5.totalprice, c=df5.c, alpha = 0.6, s=10)

plt.xlabel("Travel time, hours", size = 10)
plt.ylabel("Price, euros", size = 10)
plt.title("K-prototypes, clusters=5", size = 15)

sns.pairplot(df5,vars=['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime'],hue='cluster')

#investigate the 2d group (1 in clustering index), which has really interesting parameters
#subset the data
second = df5[df5["cluster"] == 1]
second

#only 18 from 33 transports were involved, flight was excluded from this group
#print all unique values inside

for col in second:
    print(second[col].unique())
    print(second[col].nunique())

#plot the data to reveal the mode types involved- group 2 for instance,:
#18 combinations were involved
# in general the middle price value is below 25 euros in this group, third quartile for some modes as well, to generalise- below 35
#the very max of the price would be 51 euro for the train, however for some  solutions this is the zone for extreme outliers.
# we have no flights mode for this cluster

def plot0(datt):
  plt.figure(figsize=(15, 10))
  sns.boxplot(data=datt,x="finalsolutionusedlabels", y='totalprice',palette='Set3')#take care about variables of the data
  plt.title('Price distribution per clustered groups')
  plt.xticks(rotation='vertical')
  plt.xlabel('Transport')
  plt.ylabel("Price value")


plot0(cluster_1)
plot0(cluster_2)
plot0(cluster_3)
plot0(cluster_4)
plot0(cluster_5)

df6=df5.copy()

#investigate the second group again

df6['cluster'] = list(clusters) #obtain the clusters

#again check the results
cluster_2= df6[df6['cluster']== 1]
cluster_2.head(50) #get the results

#again check the results for 1 cluster
cluster_1= df6[df6['cluster']== 0]
cluster_1.head(50) #get the results

#again check the results for 3 cluster
cluster_3= df6[df6['cluster']== 2]
cluster_3.head(50) #get the results

#again check the results for 4 cluster
cluster_4= df6[df6['cluster']== 3]
cluster_4.head(50) #get the results

#again check the results for 4 cluster
cluster_5= df6[df6['cluster']== 4]
cluster_5.head(50) #get the results

def minvalue(data):
  minvalue = data[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice']].min()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice': ")
  print(minvalue)

minvalue(cluster_1)
minvalue(cluster_2)
minvalue(cluster_3)
minvalue(cluster_4)
minvalue(cluster_5)

#investigate the group with respect to max value
def maxvalue(d):
  maxvalue = d[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice']].max()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice': ")
  print(maxvalue)

maxvalue(cluster_1)
maxvalue(cluster_2)
maxvalue(cluster_3)
maxvalue(cluster_4)
maxvalue(cluster_5)

#the range would be from 1 hour to 47 hours,0-36 waiting hours, 0-5 changes,0-0.745 walking distance, 0.72-50.840 for the price

#investigate the group with respect to max value
def meanvalue(dat):
  meanvalue = dat[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice']].mean()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice': ")
  print(meanvalue)

meanvalue(cluster_1)
meanvalue(cluster_2)
meanvalue(cluster_3)
meanvalue(cluster_4)
meanvalue(cluster_5)

#basically the centroids again

#get a 2 dimensional kernel density estimation (estimate of probability density) of cluster2 using number of changes and traveltime
# the group is not really dense havin all types of mode, mostly about publick transport and noflights, for 0 transfers the most densed area is from 3 to 13 hours
def plot1(data):
  plt.figure(figsize=(10,10))#check the size
  sns.kdeplot(x=data.totalnumberofchanges,y=data.totaltraveltimeinhours,hue=data.finiteautomaton,shade=True)
  plt.title('Density estimation per clustered groups')
  plt.xlabel('Transfers')
  plt.ylabel("Travel time")

plot1(cluster_1)
plot1(cluster_2)
plot1(cluster_3)
plot1(cluster_4)
plot1(cluster_5)

#get a 2 dimensional kernel density estimation (estimate of probability density) of cluster2 using number of changes and waiting time since they have the degree of correlation
# the group is not really dense having all types of mode, mostly about public transport ,
#public transport-waiting time is more
#the most densed area is 0-1 hour for waiting time when number of changes is 0
def plot2(data):
  plt.figure(figsize=(10,10))#check the size
  sns.kdeplot(x=data.totalnumberofchanges,y=data.totalwaitingtime,hue=data.finiteautomaton,shade=True)
  plt.title('Density estimation per clustered groups')
  plt.xlabel('Transfers')
  plt.ylabel("Waiting time")

plot2(cluster_1)
plot2(cluster_2)
plot2(cluster_3)
plot2(cluster_4)
plot2(cluster_5)

#cluster 2-observed that the majprity of distribution lies between 2 and 50 hours
# for the train mostly considered all preferences
#3 preferences the highest travel time in hours for blablacar+train, train+flixbus


def plot3(data):
  plt.figure(figsize=(20, 7))

  sns.stripplot(x=data['finalsolutionusedlabels'],y=data['totaltraveltimeinhours'],hue=data['consideredpreferences'],palette='YlGnBu')#configure the axis
  plt.xlabel("transport choices", size = 10)
  plt.xticks(rotation='vertical')
  plt.ylabel("travel time in hours", size = 10)
  plt.title('Distribution of travel time for the clustered groups',fontsize=15)

plot3(cluster_1)
plot3(cluster_2)
plot3(cluster_3)
plot3(cluster_4)
plot3(cluster_5)

#cluster 2-0 distance by foot normally having  3 combinations within preference list

def plot4(data):
  plt.figure(figsize=(20, 7))

  sns.stripplot(x=data['finalsolutionusedlabels'],y=data['totalwalkingdistance'],hue=data['consideredpreferences'],palette='bwr')#configure the axis
  plt.xlabel("transportation", size = 10)
  plt.xticks(rotation='vertical')
  plt.ylabel("walking distance", size = 10)
  plt.title('Distribution of distance by foot for the clustered groups',fontsize=15)

plot4(cluster_1)
plot4(cluster_2)
plot4(cluster_3)
plot4(cluster_4)
plot4(cluster_5)

def plot5(dat):
  fig, axes = plt.subplots(1,2,figsize=(15,10))#unpack the tuple into the variables fig and axes
#cluster 2-normally between 10 and 30
  plt.subplot(121)
  sns.boxenplot(data=dat, x='finiteautomaton',y='totalprice',palette='bright') #pick the inner parametres for plotting
  plt.title('Distribution of prices versus mode choise')
  plt.xlabel('Mode choice')
  plt.ylabel("Price")


#cluster2- we can observe logical distribution of prices depending on preferences-same pattern
  plt.subplot(122)
  sns.boxenplot(data=dat, x='consideredpreferences',y='totalprice',palette='bright')#pick the inner parametres for plotting
  plt.xticks(rotation='vertical')
  plt.title('Distribution of prices versus preference list')
  plt.xlabel('Preferences')
  plt.ylabel("Price")

plot5(cluster_1)
plot5(cluster_2)
plot5(cluster_3)
plot5(cluster_4)
plot5(cluster_5)

def plot6(data):
# top proposed solutions
  cnt_srs = data['finalsolutionusedlabels'].value_counts().nlargest(5)
  plt.figure(figsize=(15,10))
  sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="Greens_r")
  plt.title('Top proposed transportation solutions per clustered group')
  plt.xticks(rotation='vertical')
  plt.xlabel('Final solution of the algorithm per cluster', fontsize=12)
  plt.ylabel('Number of obseravtions', fontsize=12)
  plt.show()

plot6(cluster_1)
plot6(cluster_2)
plot6(cluster_3)
plot6(cluster_4)
plot6(cluster_5)

#Setting the objects to category 
categ_data = clscaled.copy()
for i in categ_data.select_dtypes(include='object'):
    categ_data[i] = categ_data[i].astype('category')

proto_labs = kproto.labels_

pip install shap

#evaluation of our clusters
#The model produced an F1 score close to 1, which means that clusters were raised are easily distinguishable
#The F1-score is the harmonic mean of precision and recall
#The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0
from lightgbm import LGBMClassifier
import shap
from sklearn.model_selection import cross_val_score
clf_kp = LGBMClassifier(colsample_by_tree=0.8)
cv_scores_kp = cross_val_score(clf_kp, categ_data, proto_labs, scoring='f1_weighted')
print(f'Cross validation F1 score for K-Prototypes clusters is {np.mean(cv_scores_kp)}')

clf_kp.fit(categ_data, proto_labs)

explainer_kk = shap.TreeExplainer(clf_kp)
shap_values_kk = explainer_kk.shap_values(categ_data)

#shap is a game theoretic approach to explain the output of any machine learning model
#Total price was significant in pushing the outcome towards cluster1 (0 with index), traveltime pushed outcomes toward cluster 2 and 5 (1 and 4 with index)
#waiting time contributed in cluster 3 (index2)
shap.summary_plot(shap_values_kk, categ_data, plot_type="bar", plot_size=(15, 10))