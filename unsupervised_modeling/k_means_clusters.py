# -*- coding: utf-8 -*-
"""k_means_clusters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rcVffS_9Mn-hOUs3sTCJPS-5wy8SJsor
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
#to include graphs nedfarrayt to the code
#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='finalnew.csv'
df = pd.read_csv(file)
print(df.describe()) #statistics

#drop redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)

"""There is an idea to perform k-means clustering on only old and newly formed numerical data (since the encoding could theoritacally distort the alghorithm which better hande combinations of numerical features only), also the column with labels also would be dropped in this case"""

km=df.copy() #make a copy of data for clustering

km.drop(km.columns[[5, 6, 7, 8, 9, 10, 12, 13, 16, 20]], axis = 1, inplace = True)# categorical columns were dropped

km # 13 numerical columns

"""check by Hopkins if the data could be clustered"""

dfhopkins=km.copy()

import sklearn 
from sklearn.preprocessing import StandardScaler
# to standardize values through scaler
scaler = StandardScaler()
# fit_transform
dfnew_scaled = scaler.fit_transform(dfhopkins)

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan
 
def hopkins(X):
    d = X.shape[1]
    #d = len(vars) # columns
    n = len(X) # rows
    m = int(0.1 * n) # heuristic from article [1]
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)
 
#generate a simulated data set (random D) drawn from a random uniform distribution with n points (q1,q2,…, qn) and the same variation as the original real data set D.
    rand_X = sample(range(0, n, 1), m)
 #compute the distance, xi, from each real point to each nearest neighbour: For each point, pi ∈ D, find it’s nearest neighbour pj; 
 #then, compute the distance between pi and pj and denote it as xi=dist(pi,pj)
    ujd = []
#compute the distance, yi from each artificial point to the nearest real data point: For each point qi ∈ random D, find it’s nearest neighbour qj in D; then compute the distance between qi and qj and denote it yi=dist(qi,qj)
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])
#calculate the Hopkins statistic (H) as the mean nearest neighbour distance in the random data set divided by the sum of the mean nearest neighbour distances in the real and across the simulated data set.
    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0
 
    return H

#convert “dfscaled” to pandas DataFrame & define previous column names 
dfnew_scaled = pd.DataFrame(dfnew_scaled)
dfnew_scaled.columns = ['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime', 'distance','numtravelfrom',	'numtravelto',	'multimodality',	'safety_boost',	'caloriesBurnt_avg',	'earnings_gross',	'delay_probability']

#apply Hopkins function to the dataset
hopkins(dfnew_scaled) #running time is 1 minute

"""Apply Hopkins function to the dataset- the clusters should be meaningful considering the numerical columns.

A function "hopkins" compares scatter spread of data points from our dataset to random scatter data which contains no cluster tendency or properties,using NearestNeighbors, meaning that the test tells us how much percentage different is our data from random scatter data We could apply Hopkins test before scaling or after the scaling as the scale of dfarray-adfarrayis & y-adfarrayis changes- it does not affect the spread of points. We conduct the Hopkins Statistic test iteratively, using 0.5 as the threshold to reject the alternative hypothesis.

If H < 0.5, then it is unlikely that data has statistically significant clusters. If the value of Hopkins statistics is close to 1, then we can reject the null hypothesis and conclude that the dataset has significantly clusterable data with respect to the columns.

Our obtained value is edfarraytremely close to 1 even taking to consideration the adding of engineered features. The numerical columns of dataframe are perfectly clusterable.
"""

#the duplicated rows after dropping the columns were removed
km.drop_duplicates(keep='last', inplace=True)
km

"""From 85153 rows we ended up with 10379 rows, to save the size of df even 8 times lower we won't be removing outliers via IQR technique, we would assess our groups that are going to encompass all the values"""

clscaled=km.copy()

from sklearn.preprocessing import MinMaxScaler # feature scaling
scaled_new = MinMaxScaler().fit_transform(clscaled[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime', 'distance','numtravelfrom',	'numtravelto',	'multimodality',	'safety_boost',	'caloriesBurnt_avg',	'earnings_gross',	'delay_probability']])
clscaled[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime', 'distance','numtravelfrom',	'numtravelto',	'multimodality',	'safety_boost',	'caloriesBurnt_avg',	'earnings_gross',	'delay_probability']] = scaled_new

#convert dataframe to numpy array
dfarray = np.array(clscaled) 
dfarray

#choose on number of clusters
from sklearn.cluster import KMeans
# amount of values to be tested for K
Ks = range(2, 10)

# list to hold on the metrics for each value of K
results = []

# running the loop
for K in Ks:
    
    model = KMeans(n_clusters = K)
    model.fit(dfarray)
    
    results.append(model.inertia_) # Inertia: Sum of distances of samples to their closest cluster center

# plot the final result
plt.plot(Ks, results, 'o-')
plt.xlabel("Values of K")
plt.ylabel("SSE")
from pylab import rcParams
rcParams['figure.figsize'] = 5, 5
plt.show()

pip install kneed

from kneed import KneeLocator
kl = KneeLocator(
        range(2, 10), results, curve="convex", direction="decreasing")

kl.elbow

"""We can hypothetically choose the number of clusters as 5"""

from yellowbrick.cluster import KElbowVisualizer

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]
for n_clusters in range_n_clusters:
    #initializing the clusterer with n_clusters value and a random generator
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(dfarray)
    #using Elbow Plot
    visualizer= KElbowVisualizer(clusterer,k=(2,10),
    metric  ='calinski_harabasz',locate_elbow=False, timings= False) #usage of another metric
    #fitting the data to the visualizer
    visualizer.fit(dfarray)  
    #render the figure
    visualizer.show()

"""The Calinski-Harabasz index also known as the Variance Ratio Criterion, is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters, the higher the score , the better the performances.
The technique proposes to choose the number of clusters as 2 (only 2 groups)
"""

silhouette_scores = [] 

for n_cluster in range(2, 10):
    silhouette_scores.append( 
        silhouette_score(dfarray, KMeans(n_clusters = n_cluster).fit_predict(dfarray))) 
    
# Plotting a bar graph to compare the results 
k = [2, 3, 4, 5, 6, 7, 8, 9] 
plt.bar(k, silhouette_scores) 
plt.xlabel('Number of clusters', fontsize = 10) 
plt.ylabel('Silhouette Score', fontsize = 10) 
plt.show()

range_n_clusters = list (range(2,10))
for n_clusters in range_n_clusters:
    clusterer = KMeans(n_clusters=n_clusters)
    preds = clusterer.fit_predict(dfarray)
    centers = clusterer.cluster_centers_

    score = silhouette_score(dfarray, preds)
    print("For n_clusters = {}, silhouette score is {})".format(n_clusters, score))

"""the graph based on fit_predict also suggests using 2 (the highest Silhouette) - apply 2 clusters to get only 2 groups"""

from sklearn.cluster import KMeans

# creating the model
#kmeans = KMeans(n_clusters = 5, verbose=1)
kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42, verbose=1)

# training our model and transform to cluster-distance space
trans= kmeans.fit_transform(dfarray)

# edfarrayplore the labels (clusters) assigned for each data point with the function labels_
kmeans.labels_

# assigning the labels to the not scaled dataset
km['kmeans_cluster'] = kmeans.labels_

km#check the assigned clusters

km.kmeans_cluster.unique() #make sure that only 2 groups were assigned

"""only 2 different groups as 2 techniques proposed"""

k=km.copy()

#drop redundant column via reading
k.drop('kmeans_cluster', axis=1, inplace=True)

dfnotscaled=np.array(k)
dfnotscaled

#plotting through PCA
from sklearn.decomposition import PCA
import pylab as pl

# reducing data dimensions 
PCA_ = PCA(n_components = 2).fit(clscaled) #for the sake of visualisation 

# applying the PCA
PCA_2 = PCA_.transform(clscaled)


# plot size
from pylab import rcParams
rcParams['figure.figsize'] = (8.0, 8.0)

# plotting each point individually depending on their cluster
for i in range(0, PCA_2.shape[0]):
    
    # If the 'i' data point is in cluster 0, it will be plotted as the formatting inside the if functions
    
    if kmeans.labels_[i] == 0:
        CLUSTER_01 = pl.scatter(PCA_2[i,0], PCA_2[i,1], c ='r', marker = 'o', s = 120)
        
    elif kmeans.labels_[i] == 1:
        CLUSTER_02 = pl.scatter(PCA_2[i,0], PCA_2[i,1], c ='g', marker = 'o', s = 120)

    #elif kmeans.labels_[i] == 2:
        #CLUSTER_03 = pl.scatter(PCA_2[i,0], PCA_2[i,1], c ='b', marker = 'o', s = 120)

    #elif kmeans.labels_[i] == 3:
        #CLUSTER_04 = pl.scatter(PCA_2[i,0], PCA_2[i,1], c ='y', marker = 'o', s = 120)

    #elif kmeans.labels_[i] == 4:
        #CLUSTER_05 = pl.scatter(PCA_2[i,0], PCA_2[i,1], c ='c', marker = 'o', s = 120)
        
        
# formatting the plot
pl.legend([CLUSTER_01, CLUSTER_02],
                  ['Cluster 01', 'Cluster 02'])
pl.title('Clustered groups')
        
pl.show()

"""through PCA technique the graph is not really self-explanatory"""

t=clscaled.copy() #for parallel_plot

t['kmeans_cluster'] = kmeans.labels_ #assugn the labels

#make the plot to get insights about the clusters
from pandas.plotting import parallel_coordinates #better for k-means visualization
plt.figure(figsize=(30,10))
parallel_coordinates(t, 'kmeans_cluster', colormap=plt.get_cmap("Set1")) #check the data and hue, set the color ranges
plt.title("Group visualization according to cluster label assigned")
plt.xlabel("Dataset features including incorporated")
plt.ylabel("ratio")
plt.savefig('graph.png')
plt.show()

"""2 groups:
red and grey

red: expensive long-distanced trips with low and up to moderate (50-50 chance of delay)
grey- trips that include high walking distance,more than 1 mode involved however its possible to earn money wihout being distracted and of course reduce the calories. The highest chance of the delay.
"""

#plotting with PCA again
from sklearn.decomposition import PCA
import pylab as pl

pca = PCA(2) 
pca_data = pd.DataFrame(pca.fit_transform(clscaled),columns=['PC1','PC2']) 
pca_data['cluster'] = pd.Categorical(kmeans.labels_)
sns.scatterplot(x="PC1",y="PC2",hue="cluster",data=pca_data)

Same configuration

# the results for 1 and 2 groups
cluster_1= km[km['kmeans_cluster']== 0]
cluster_1.head(20) #get the results

# the results for 1 and 2 groups
cluster_2= km[km['kmeans_cluster']== 1]
cluster_2.head(20) #get the results

kmeans.cluster_centers_ #getting the centroids

c = pd.DataFrame(kmeans.cluster_centers_)
c.columns = ['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime', 'distance','numtravelfrom',	'numtravelto',	'multimodality',	'safety_boost',	'caloriesBurnt_avg',	'earnings_gross',	'delay_probability']

#append column cluster to the centers space
ss = pd.Series(['1','2'], index=[0, 1])
c['cluster'] = ss.values

#assess the centers
c

#investigate the groups with respect to mean value

def meanvalue(dat):
  meanvalue = dat[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime', 'distance','numtravelfrom',	'numtravelto',	'multimodality',	'safety_boost',	'caloriesBurnt_avg',	'earnings_gross',	'delay_probability']].mean()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice'& 'distance' & 'numtravelfrom'&	'numtravelto'&'multimodality' & 'safety_boost' & 'caloriesBurnt_avg' &	'earnings_gross' &'delay_probability': ")
  print(meanvalue)

meanvalue(cluster_1)
meanvalue(cluster_2)

"""1st group is more expensive, higher distance

2d- higher walking distance, total multimodality, travellers earn more, high ratio of calories to burn, highest delay ratio

Copy of the logics from parallel plot
"""