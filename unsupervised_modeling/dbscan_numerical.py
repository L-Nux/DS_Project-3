# -*- coding: utf-8 -*-
"""DBSCAN_numerical.ipynb
 
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MIP9849pNoavS_oTeI4Q5U-USaaP7TEa
"""

#prepare for the other combinations for DBSCAN=>numerical 'travelto' and 'travelfrom'

#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='2featuresout.csv'
df = pd.read_csv(file)
print(df.describe()) #statistics

#incorporate the dataset meta_city_size for changing the approach with the columns with respect to population
file='metacitysize.csv'
dfcity = pd.read_csv(file)
print(dfcity.describe())

dfcity#check the dataset

#create 2 new numerical columns travel from and travel to with respect to population size of the cities for the customer
df['numtravelfrom'] = "null"
df['numtravelto'] = "null"

df

#populate our common dataframe with + numerical features based on aligment with another df regarding cities:
for i in range(0, len(df)):
  for j in range(0, len(dfcity)):
    if df ["sourcename"].values[i] == dfcity ["city"].values[j]:
      df['numtravelfrom'].values[i] = dfcity ["population_total"].values[j]
    elif df ["targetname"].values[i] == dfcity ["city"].values[j]:
      df['numtravelto'].values[i] = dfcity ["population_total"].values[j]

#check the frame
df

"""Would NOT reduce the outliers since the DBSCAN is supposed to catch them via -1 cluster, it could properly handle the noise"""

#mapping to numerical population-correct?check the numbers!
df.loc[df['sourcename'] == 'Hamburg']

"""fully alligned with the dataset"""

#redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)

df

#no missing values
df.isna().sum()

df.drop(df.columns[[5, 9, 10, 12, 13]], axis = 1, inplace = True) #drop 5 columns +2 additionally

"""Define what columns we need to drop for the clustering

goal is to cluster the group of trips and see maybe we can detect some new patterns in the data based on clusters, was decided to drop column objective-super low degree of association-0,05.. doesnt have any meaningfull information for clustering, we have completely identical rows with all numerical/categorical variables (having the same labesls) with difference only in objective=> it doesnt change anything, it also repeats somehow the combinations inside the 'considered preferences" column, whoch we will use for clustering

Drop sourcename and targetname for the algorithm- we would need to consider the groups of trips in Germany in general, use the same dropping approach as for kprototypes

PLUS also categorical travelfrom and travelto got dropped-we will use numerical replacements to check the behavious of the alghorithm
"""


#the duplicates were removed
df.drop_duplicates(keep='last', inplace=True)

"""From 85153 to 30486 thousands of rows (mainly because of reducing the objective column) +20 rows in comparison to datatset with categorical(travel from and travel to)"""

#save dataset for KNIME to compare later in Knime dbscan behaviour (included in report)
from google.colab import files
df.to_csv('num2featuresout.csv') 
files.download('num2featuresout.csv')

#copy of data for encoding
encoding=df.copy()

from sklearn.preprocessing import LabelEncoder #encode the values
labelencoder = LabelEncoder()

#use label encoding for the column considered preferences because it has the order
encoding['consideredpreferences'] = labelencoder.fit_transform(encoding['consideredpreferences'])#fit label encoder and return encoded labels for the first target column
#use label encoding because we have too many values for one hot encoding for solutionlabels-33 unique combinatios
encoding['finalsolutionusedlabels'] = labelencoder.fit_transform(encoding['finalsolutionusedlabels'])#Fit label encoder and return encoded labels for the second one


encoding.head(10)

#use all the features to cluster the observations
categorical_col= ["finiteautomaton"] #no order-usage of dummy encoding
transformed = pd.get_dummies(encoding, columns=categorical_col)#convert categorical target our automation

#check what we have got as the result
transformed.head(10)

"""Got our transformed dataset to plug it into dbscan algorithm and check the clusters"""

#check the dataset
transformed

#transformation
transformed['numtravelfrom'] = pd.to_numeric(transformed['numtravelfrom'])
transformed['numtravelto'] = pd.to_numeric(transformed['numtravelto'])

"""The problem was that the datatype of 'numtravelfrom' and 'numtravelto' are objects and we cant investigate the correlations=we transform them to integers"""

correlation_df = transformed.corr()
#calculate all correlations in df via table

correlation_df

#check again the datatypes
transformed.dtypes

#create the second correlation matrix using our transformed categorical data plus numerical data
show=transformed.corr(method='kendall')#pairwise correlation
plt.figure(figsize=(10,10))
sns.heatmap(show,annot=True,cmap='PuOr')

"""Strong negative correlation between preferences and walikngdistance&waitingtime&transfers=> our minimisation boundaries, new correlation between distance and traveltime(moderate degree) and distance and price(high degree) was revealed, super small correlation for numerical travelfrom and travelto population size"""

#import further libraries for DBscan
from sklearn.preprocessing import MinMaxScaler # for feature scaling
from sklearn import metrics # for calculating Silhouette score

import matplotlib.pyplot as plt # for data visualization
import plotly.graph_objects as go # for data visualization
import plotly.express as px # for data visualization

from sklearn.cluster import DBSCAN

#to scale the features utilized min-max scaler because data distribution is not normal (not fit a gaussian distribution)
scaler = MinMaxScaler()
transformed_scaled = scaler.fit_transform(transformed) #apply the Scaler

# plot the min-max scaled distributions (dataset is combined)

fig, axs = plt.subplots(1, 8, figsize=(40,10), dpi=300)
axs[0].hist(transformed_scaled[:,0], bins=50, color='blue', rwidth=0.9)
axs[0].set_title('totaltraveltimeinhours')
axs[1].hist(transformed_scaled[:,1], bins=50, color='blue', rwidth=0.9)
axs[1].set_title('totalprice')
axs[2].hist(transformed_scaled[:,2], bins=50, color='green', rwidth=0.9)
axs[2].set_title('totalnumberofchanges')
axs[3].hist(transformed_scaled[:,3], bins=50, color='green', rwidth=0.9)
axs[3].set_title('totalwalkingdistance')
axs[4].hist(transformed_scaled[:,4], bins=50, color='green', rwidth=0.9)
axs[4].set_title('totalwaitingtime')
axs[5].hist(transformed_scaled[:,7], bins=50, color='yellow', rwidth=0.9)
axs[5].set_title('distance')
axs[6].hist(transformed_scaled[:,8], bins=50, color='red', rwidth=0.9)
axs[6].set_title('numtravelfrom')
axs[7].hist(transformed_scaled[:,9], bins=50, color='red', rwidth=0.9)
axs[7].set_title('numtravelto')

plt.show()

"""Control that all of them are between 1 and 0, including the newest incorporated feature 'distance', as well as numerical population from travelling from and to"""

#verify the final array
transformed_scaled

#check the shape
transformed_scaled.shape

"""30486 rows and 13 columns, including encoded ones

If data has more than 2 dimensions, choose MinPts = 2*dim, where dim= the dimensions of your data set (Sander et al., 1998)=13*2=26
"""

from sklearn.neighbors import NearestNeighbors #for NN approach
from matplotlib import pyplot as plt
#selected 26minpts
neighbors = NearestNeighbors(n_neighbors=26) #13*2
neighbors_fit = neighbors.fit(transformed_scaled)
distances, indices = neighbors_fit.kneighbors(transformed_scaled)

"""To elaborate more on epsilon calculate the average distance between each point in the data set and its 26th nearest neighbors ( selected minpts value)

references https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd
"""

#sort distance values by ascending value and plot

distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)

"""Assume that optimal eps would be at the max curvature- 0.08-0.15 - we will try a loop"""

#try the second approach
neigh = NearestNeighbors(n_neighbors=2)
nbrs = neigh.fit(transformed_scaled)
distances, indices = nbrs.kneighbors(transformed_scaled)

"""Calculate the distance from each point to its closest neighbour using the NearestNeighbors, the point itself is included in n_neighbors

The kneighbors method returns two arrays, one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points

references https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc
"""

#sort and plot results
distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)

"""optimal eps should be 0,08-0,15 and we will also try a loop with different combinations to compare Silhouette there"""

#test different hyperparameter values for DBscan- the highest Silhouette score -we will use for clustering these parameters
# empty lists

SS=[] # to store inside different Silhouette scores
# to save results of various epsilon and min_samples
combinations=[] 

# ranges to choose
# we will choose different suitables ranges to test
eps_range=range(10,13) # 0.1 - 0,13 range

minpts_range=range(26,28) #rule of thumb-choose based on number of dimensions (dim+1 as min), or even more dim*2

for k in eps_range:
    for j in minpts_range:
        # model and parameters
        model = DBSCAN(eps=k/100, min_samples=j)
        # fitting the model 
        clm = model.fit(transformed_scaled)
        # calculation of Silhoutte Score and appending it to the list
        SS.append(metrics.silhouette_score(transformed_scaled, clm.labels_, metric='euclidean'))
        combinations.append(str(k)+"|"+str(j)) # axis values for the graph

# plotting the results of Silhouette scores to make a decision
plt.figure(figsize=(35,8), dpi=300)
plt.plot(combinations, SS, 'bo-', color='black')
plt.xlabel('Epsilon/100 | MinSample Z')
plt.ylabel('Returned Silhouette Coefficient')
plt.title('Silhouette Score with the various combinations of hyperparameters')
plt.show()

"""Running time is 1 minute, Silhouette is negative

"""

#test different hyperparameter values for DBscan- the highest Silhouette score -we will use for clustering these parameters
# empty lists

SS=[] # to store inside different Silhouette scores
# to save results of various epsilon and min_samples
combinations=[] 

# ranges to choose
# we will choose different suitables ranges to test
eps_range=range(80,82) # 0.8 - 0,82 range

minpts_range=range(259,263) #rule of thumb-choose based on number of dimensions (dim+1 as min), or even more dim*2

for k in eps_range:
    for j in minpts_range:
        # model and parameters
        model = DBSCAN(eps=k/100, min_samples=j)
        # fitting the model 
        clm = model.fit(transformed_scaled)
        # calculation of Silhoutte Score and appending it to the list
        SS.append(metrics.silhouette_score(transformed_scaled, clm.labels_, metric='euclidean'))
        combinations.append(str(k)+"|"+str(j)) # axis values for the graph

# plotting the results of Silhouette scores to make a decision
plt.figure(figsize=(35,8), dpi=300)
plt.plot(combinations, SS, 'bo-', color='black')
plt.xlabel('Epsilon/100 | MinSample Z')
plt.ylabel('Returned Silhouette Coefficient')
plt.title('Silhouette Score with the various combinations of hyperparameters')
plt.show()

"""Running time is 3 minutes again using the same greed search approach plotting versus Silhouette and looping, the best coefficient 0,497-we will use eps=0,81 and minpts=262"""

#eventually use for the model: eps=0.81, MinPts=262 (almost the same when the categorical encoded columns were involved)

modeldb = DBSCAN(eps=0.81, # default=0.5, The maximum distance between two samples for one to be considered as in the neighborhood of the other.
               min_samples=262, # default=5, The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.
               metric='euclidean', # default='euclidean'. The metric to use when calculating distance between instances in a feature array. 
               metric_params=None, # default=None, Additional keyword arguments for the metric function.
               algorithm='auto', # {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’, The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors.
               leaf_size=30, # default=30, Leaf size passed to BallTree or cKDTree.
               p=None, # default=None, The power of the Minkowski metric to be used to calculate distance between points. If None, then p=2
               n_jobs=None, # default=None, The number of parallel jobs to run. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.
              )



# fit our model
clustm = modeldb.fit(transformed_scaled)


# check the results
print('DBSCAN Clustering Model ')
print("Cluster labels for the first model")
print(clustm.labels_)

"""3 clusters were formed, having no -1 means that we have no noise being identified by the algorithm, 3 groups"""

#assign clusters to the data frame for further analysis of groups via plots
df['DBSCAN cluster']=clustm.labels_
df

#3 groups again 
a = df['DBSCAN cluster'].unique()
a

"""no noise points were assigned"""

# plot the clusters in 3d environment with the colors assigned per cluster
df=df.sort_values(by=['DBSCAN cluster'])

# 3d scatter plot
fig = px.scatter_3d(df, x=df['totalprice'], y=df['totaltraveltimeinhours'], z=df['distance'], 
                    opacity=1, color=df['DBSCAN cluster'].astype(str), 
                    color_discrete_sequence=['black']+px.colors.qualitative.Plotly,
                    hover_data=['totalwalkingdistance', 'totalwaitingtime'],
                    width=900, height=900
                   )

# chart
fig.update_layout(#title_text="Scatter 3D Plot",
                  showlegend=True,
                  legend=dict(orientation="h", yanchor="bottom", y=0.04, xanchor="left", x=0.1),
                  scene_camera=dict(up=dict(x=0, y=0, z=1), 
                                        center=dict(x=0, y=0, z=-0.2),
                                        eye=dict(x=1.5, y=1.5, z=0.5)),
                                        margin=dict(l=0, r=0, b=0, t=0),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         ),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                          ),
                               zaxis=dict(backgroundcolor='lightgrey',
                                          color='black', 
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         )))
# change marker size
fig.update_traces(marker=dict(size=2))

fig.show()

"""With the silhouette almost 0,5 we can conclude that clusters are not that indifferent , distance between them could be significant

1st black group moderate with respect to 3 parameters

hard to evaluate 2 other wrt to 3 dim graph
"""

# plot the clusters in 3d environment with the colors assigned per cluster
df=df.sort_values(by=['DBSCAN cluster'])

# 3d scatter plot
fig = px.scatter_3d(df, x=df['totalprice'], y=df['totaltraveltimeinhours'], z=df['totalnumberofchanges'], #usage of another third column
                    opacity=1, color=df['DBSCAN cluster'].astype(str), 
                    color_discrete_sequence=['black']+px.colors.qualitative.Plotly,
                    hover_data=['totalwalkingdistance', 'totalwaitingtime'],
                    width=900, height=900
                   )

# chart
fig.update_layout(#title_text="Scatter 3D Plot",
                  showlegend=True,
                  legend=dict(orientation="h", yanchor="bottom", y=0.04, xanchor="left", x=0.1),
                  scene_camera=dict(up=dict(x=0, y=0, z=1), 
                                        center=dict(x=0, y=0, z=-0.2),
                                        eye=dict(x=1.5, y=1.5, z=0.5)),
                                        margin=dict(l=0, r=0, b=0, t=0),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         ),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                          ),
                               zaxis=dict(backgroundcolor='lightgrey',
                                          color='black', 
                                          gridcolor='#f0f0f0',
                                          title_font=dict(size=10),
                                          tickfont=dict(size=10),
                                         )))
# change marker size
fig.update_traces(marker=dict(size=2))

fig.show()

"""With the silhouette almost 0,5 we can conclude that clusters are not that indifferent , distance between them could be significant, more changes for the second group,less travel time, but high price, less changes for the 3d group, travel time is hogh but the price is moderate, black first group-not high price and moderate traveltime&changes"""

#check how many instances belong to the cluster
print(pd.Series(clustm.labels_).value_counts())

"""second group is the biggest"""

df['DBSCAN cluster']=clustm.labels_

#0,1,2 our 3 groups
clustm.labels_

#to plot 2 dimensions
dbscan=DBSCAN(eps=0.81, min_samples=262)

lables=dbscan.fit_predict(transformed_scaled)

#2 dimensions with scaled clusters
lables=dbscan.fit_predict(transformed_scaled)
plt.figure(figsize=(10, 10))
plt.scatter(transformed_scaled[lables == 0, 0], transformed_scaled[lables == 0, 1], s = 50, c = 'black') #use the color
plt.scatter(transformed_scaled[lables == 1, 0], transformed_scaled[lables == 1, 1], s = 50, c = 'violet')
plt.scatter(transformed_scaled[lables == 2, 0], transformed_scaled[lables == 2, 1], s = 50, c = 'red')

plt.xlabel('Travel time')
plt.ylabel('Price')
plt.title('DBSCAN other combinations 3 Clusters of data')
plt.show()

"""Highest price for pink-second (violet), highest travel time for third (red), moderate first(black)"""

#pairwise 3 clusters unscaled

sns.pairplot(df,vars=['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime', 'distance', 'numtravelfrom', 'numtravelto'],hue='DBSCAN cluster', palette ='Blues')

"""Again 0 (1st) cluster is moderate, 2d cluster (index 1) highest orice, 3d cluster highest travel time- dark blue

Groups are also shuffled, not distinct with respect to pairwise correlaion graph

"""

#three unique clusters
np.unique(lables)

#form the group for cluster 1 (moderate group)

cluster_1= df[df['DBSCAN cluster']== 0]
cluster_1.head(20) #get the results

"""mode-all"""

#form the group for cluster 2

cluster_2= df[df['DBSCAN cluster']== 1]
cluster_2.head(20) #get the results

"""price is higher (expensive group)"""

#form the group for cluster 3

cluster_3= df[df['DBSCAN cluster']== 2]
cluster_3.head(20) #get the results

"""price is lower"""

def minvalue(data):
  minvalue = data[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice', 'distance', 'numtravelfrom', 'numtravelto' ]].min()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice'& 'distance'& 'numtravelfrom'& 'numtravelto': ")
  print(minvalue)

minvalue(cluster_1)
minvalue(cluster_2)
minvalue(cluster_3)

#investigate the group with respect to max value
def maxvalue(d):
  maxvalue = d[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice', 'distance', 'numtravelfrom', 'numtravelto']].max()
  print("maximum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice' &'distance'& 'numtravelfrom'& 'numtravelto': ")
  print(maxvalue)

maxvalue(cluster_1)
maxvalue(cluster_2)
maxvalue(cluster_3)

#investigate the group with respect to mean value

def meanvalue(dat):
  meanvalue = dat[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice', 'distance', 'numtravelfrom', 'numtravelto']].mean()
  print("mean value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice': ")
  print(meanvalue)

meanvalue(cluster_1)
meanvalue(cluster_2)
meanvalue(cluster_3)

#values in general

"""no distinctive mean"""

#density graphs

def plot1(data):
  plt.figure(figsize=(10,10))#check the size
  sns.kdeplot(x=data.totalnumberofchanges,y=data.totalwaitingtime,hue=data.finiteautomaton,shade=True)
  plt.title('Density estimation per clustered groups')
  plt.xlabel('Transfers')
  plt.ylabel("Waiting time")

plot1(cluster_1)
plot1(cluster_2)
plot1(cluster_3)

"""1st and 3d group-0 transfers-noflights
cant detect the mode of all as dense one

1st group with 2 trabsfers the highest time for public transport only
"""

def plot2(data):
# top proposed solutions
  cnt_srs = data['finalsolutionusedlabels'].value_counts().nlargest(3)
  plt.figure(figsize=(15,10))
  sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="Oranges")
  plt.title('Top proposed transportation solutions per clustered group')
  plt.xticks(rotation='vertical')
  plt.xlabel('Final solution of the algorithm per cluster', fontsize=12)
  plt.ylabel('Number of observations', fontsize=12)
  plt.show()

plot2(cluster_1)
plot2(cluster_2)
plot2(cluster_3)

"""Same top transport labels everywhere (train and flixbus combinations)"""

#plot the data to reveal the mode types per city of going to

def plot3(datt):
  plt.figure(figsize=(15, 10))
  sns.boxplot(data=datt,x="finalsolutionusedlabels", y='numtravelto',palette='copper')#take care about variables of the data
  plt.title('Population per city and transport labels')
  plt.xticks(rotation='vertical')
  plt.xlabel('Transport')
  plt.ylabel("Population per city of arrival,mln")


plot3(cluster_1)
plot3(cluster_2)
plot3(cluster_3)

"""all mix of transport solutions inside, all groups travel to metropolis via same 2 modes-train, blablacar,flixbus and combinations"""
