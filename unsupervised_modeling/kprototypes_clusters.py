# -*- coding: utf-8 -*-
"""Kprototypes clusters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XyBIomVNZMbNDN8Mcl6jnUdlOAprMqM3
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
#to include graphs next to the code
#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='2featuresout.csv'
df = pd.read_csv(file)
print(df.describe()) #statistics

df#check dataset

#drop redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)

reduce=df.copy() #a copy for reducing

q1 = reduce['totalprice'].quantile(0.25)
q3 = reduce['totalprice'].quantile(0.75)
iqr = q3 - q1     

#some extreme data values have been removed
filter = (reduce['totalprice'] >= (q1 - 1.5 * iqr)) & (reduce['totalprice'] <= (q3 + 1.5 *iqr)) #make the range
reducenew=reduce.loc[filter]#filtering the data

reducenew.head(10) #print 10 rows

"""Drop extreme outliers for price and traveltime, make k-prototypes more robust. 

Having dummies 0s (NaN actually) in other 3 numerical columns (changes,distance,waiting time) and trying to drop outliers- we are loosing more, than half of dataset in total

Deciced to keep all values for other 3 numerical columns apart from 'price' and 'travel time', where we have precise estimations

Dropping the outliers of price using IQR method-interquartile range, statistically, it is assumesd that the values are clustered around some central value, i.e. IQR = Q3 – Q1 . If a data point is below Q1 – 1.5×IQR or above Q3 + 1.5×IQR, it is considered as being too far from the central values
"""

reducenew #check new

"""Reducing the extreme price from 85153 to 84786. For dbscan approach we would use all 85153 to check if we have the noise specifically"""

mm = reducenew[['totalprice']].max()#check the max value, previously it was 360+
mm

#reduce for travel time

q1 = reduce['totaltraveltimeinhours'].quantile(0.25)
q3 = reduce['totaltraveltimeinhours'].quantile(0.75)
iqr = q3 - q1     

#some extreme data values have been removed
filter = (reduce['totaltraveltimeinhours'] >= (q1 - 1.5 * iqr)) & (reduce['totaltraveltimeinhours'] <= (q3 + 1.5 *iqr))
reducenew=reduce.loc[filter]#filtering the data

reducenew.head(10)

#doublecheck for missings
reducenew.isna().sum()

"""Do not have the missing values explicitely (only 0s inside transfers, walking distance and waiting time- for some column (waiting time it would be more than a half-50+thousands of NaNs, decided not to drop them or not to use KNN imouter to distirt the dataset with many estimated values)"""

#save the newest df for kprototypes and sharing
from google.colab import files
reducenew.to_csv('kprot2features.csv') 
files.download('kprot2features.csv')

#check the updated version
reducenew

"""From 84786 to 81142 finally with outliers for travelling time reduced"""

dff=reducenew.copy()# for clustering

dff.drop(dff.columns[[5, 9, 10]], axis = 1, inplace = True)# 3 columns gor dropped

"""Define what columns we need to drop for the kprototypes clustering

The goal is-cluster the group of trips and evaluate if we can detect some special patterns based on groups (also with additional incorporated features)

Was decided to drop column 'objective'-super low degree of cramers association-0,05.. doesnt have any meaningfull information for clustering

We have completely identical rows with all numerical/categorical variables (having the same labesls) with difference only in objective=> it doesnt change anything
The objective value also got repeated in the combinations inside the 'considered preferences" column-our constraints for minimisation, which I will keep for groupping

Drop 'sourcename' and 'targetname' with the precise city name for the algorithm, would consider the groups of trips in Germany in general
"""

dff

#the duplicated rows after dropping the columns were removed
dff.drop_duplicates(keep='last', inplace=True)

#the version after dropping
dff

"""From 81142 to 28652 thousands of rows (mainly because of eliminating the objective column)"""

#check if the silution labels got shrinked
dff['finalsolutionusedlabels'].unique()

"""Saved all unique solution labels"""

#check the data types
dff.info()

"""For k.prototypes we need float as the input for numerical=the requirement of alghorithm"""

#for scaling
clscaled=dff.copy()

"""The numerical columns are within the different range we need to scale them for clustering- use minmaxscaler instead of Standard Scaler"""

from sklearn.preprocessing import MinMaxScaler # feature scaling
scaled_new = MinMaxScaler().fit_transform(clscaled[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime', 'distance']])
clscaled[['totaltraveltimeinhours', 'totalprice','totalnumberofchanges','totalwalkingdistance','totalwaitingtime', 'distance']] = scaled_new

#check if everything got scaled by MINMAX
clscaled

"""Scales and translates each feature individually such that it is in the given range on the training set,between zero and one"""

#detect categorical columns
catColumnsPos = [dff.columns.get_loc(col) for col in list(clscaled.select_dtypes('object').columns)]
print('Categorical columns           : {}'.format(list(clscaled.select_dtypes('object').columns)))
print('Categorical columns position  : {}'.format(catColumnsPos))

"""Get the position of categorical columns, which would be handled by k-prototypes
Use preference, automation, labels, travelfrom and travelto
"""

#convert dataframe to matrix
dfmatrix = clscaled.to_numpy()
dfmatrix

"""Need it for kprototypes input as well as floats"""

#install the library
!pip install kmodes

#to check how many clusters to choose
from tqdm import tqdm
from kmodes.kprototypes import KPrototypes
from matplotlib import style
#running time is 2+hours to obtain the optimal number of clusters
costs = []
n_clusters = []
clusters_assigned = []
# specify categorical input with added features regarding the city size group-columns 9 and 10
cat_cols = [5,6,7,9,10]

for i in tqdm(range(2, 10)):
    try:
        kproto = KPrototypes(n_clusters=i, init='Huang', verbose=2) #to print iterations
        clusters = kproto.fit_predict(dfmatrix, categorical=cat_cols)#use the list of columns
        costs.append(kproto.cost_)
        n_clusters.append(i)
        clusters_assigned.append(clusters)
    except:
        print(f"Can't cluster with {i} clusters")

#plot the Elbow curve

from plotly import graph_objects as go
fig = go.Figure(data=go.Scatter(x=n_clusters, y=costs))
fig.update_layout(
    title="Elbow method",
    xaxis_title="Clusters",
    yaxis_title="Costs",
    font=dict(
        family="Courier New, monospace",
        size=18,
        color="#7f7f7f"
    )
)
fig.show()

"""Obtained the result that the best number for clusters is 4 (also could be 5 having in mind that we have 5  transports invloved-plain,train,bus,car,flixbus,but the line is not that steep for 5 of them

Will cluster with 4 clusters based on the curve (after 4 the line is 'calmer')

"""

style.use("ggplot")
colors = ['b', 'g', 'r','y','m']  #assign the color to cluster

cat_cols = [5,6,7,9,10]
kproto = KPrototypes(n_clusters=4, init='Huang', verbose=2) #clustering itself
clusters = kproto.fit_predict(dfmatrix, categorical=cat_cols)#usage of constracted matrix with scaled numericals +categorical columns

# print cluster centroids of the unsupervised model
print(kproto.cluster_centroids_)

# print training statistics
print(kproto.cost_)
print(kproto.n_iter_)

#check how many observations we have in each cluster
print(pd.Series(clusters).value_counts())

"""2,3,4 clusters have +- same assigned points, the first one (0 index) has the minority of assigned points"""

#create dict for clusters
cluster_dict=[]
for c in clusters:
  cluster_dict.append(c)

#get all the values of clusters proposed wrt individual records
cluster_dict

# go to the initial dataframe dff and assign number of clusters
#copy df for 4 clusters
df4=dff.copy()

#create a column

df4['cluster']=cluster_dict #attach clusters to original dataframe df to see what separate groups we got

#check the clusters in the new column 'cluster'
df4

#get the fisrt 20 values for cluster 0 to understand a rough trend

df4[df4['cluster']==0].head (20)

"""Cluster 1(index0)-more to walk from the first sight, some variety in the columns"""

#print the cluster centroids
print(kproto.cluster_centroids_)

#convert “dfscaled” to pandas DataFrame & define previous column names 
centr = pd.DataFrame(kproto.cluster_centroids_)
centr.columns =['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime', 'distance', 'finiteautomaton',	'consideredpreferences',	'finalsolutionusedlabels', 'travelfrom', 'travelto']

#append column cluster to the centroids space
s = pd.Series(['1','2', '3','4'], index=[0, 1, 2, 3])
centr['cluster'] = s.values

#evaluate the centroids
centr

"""1st cluster with index 0(long, not-convenient trips)- more time to travel,highest number of changes,highest distance to walk,highest waiting time, also flixbus involved, however the distance between 2 cities is not the highest one

2d cluster with index 1(almost no walking)- the least walking distance

3d cluster with index 2 (expensive trips)- the highest price (expensive trips), moderate number of transfers, the highest distance between 2 cities to cover, which could explain the price ranges

4th cluster with index 3(short and cheap trips)- the fastest trips with the lowest price, the smallest number of changes, the smallest waiting time and the short distance between 2 cities to cover-which explains the parameters
"""

colors = ['b', 'g', 'r', 'y']
df4['c'] = df4.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3]}) # define and repeat colors to visulize the clusters via separate graph

#plot 4 groups
plt.figure(figsize=(10, 10))
plt.scatter(df4.totaltraveltimeinhours, df4.totalprice, c=df4.c, alpha = 0.6, s=10)

plt.xlabel("Travel time, hours", size = 10)
plt.ylabel("Price, euros", size = 10)
plt.title("K-prototypes, clusters=4", size = 15)

"""Can conclude that the clusters were defined, the data got clustered in the distinguishable way even without PCA after clustering for plotting,
Just plotted per two main dimensions price and travel time

yellow short and cheap, red-expensive trips, blue- with the highest travel time
"""

#use pairplot for further investigation

sns.pairplot(df4,vars=['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime', 'distance'],hue='cluster', palette ='Set1_r')

"""Again define and descibe the clusters with respect to column distance horizontal axis-travel timeinhours-vertical

4th cluster(red) with the lowest distance between OD
3d cluster(green) with the highets distance between OD
"""

#investigate the 4th group (3 in clustering index)
#subset the data
fourth = df4[df4["cluster"] == 3]
fourth

#investigate 4th group

for col in fourth:
    print(fourth[col].unique())
    print(fourth[col].nunique())

"""Up to 4 changes are involved,all finiteautomationmodes,all considered preferences,23 from 33 labels, basically all OD pairs"""

df5=df4.copy()#new df for the list

df5['cluster'] = list(clusters) #obtain the clusters for the copy

#arrange the separate pieces of datasets for further plotting
cluster_2= df5[df5['cluster']== 1]
cluster_2.head(50) #get the results

#again check the results for 1 cluster
cluster_1= df5[df5['cluster']== 0]
cluster_1.head(50) #get the results

#the results for 3 cluster
cluster_3= df5[df5['cluster']== 2]
cluster_3.head(50) #get the results

# the results for 4 cluster
cluster_4= df5[df5['cluster']== 3]
cluster_4.head(50) #get the results

"""Price is cheap and time is fast indeed"""

#plot the data to reveal the mode types involved:

def plot0(datt): #function
  plt.figure(figsize=(15, 10))
  sns.boxplot(data=datt,x="finalsolutionusedlabels", y='distance',palette='Set3')#take care about variables of the data
  plt.title('Distance between OD per clustered groups and transport labels')
  plt.xticks(rotation='vertical')
  plt.xlabel('Transport')
  plt.ylabel("Distance,km")


plot0(cluster_1)
plot0(cluster_2)
plot0(cluster_3)
plot0(cluster_4)

"""4th group as we can see the least distance (up to 350 km instead of up to 600, no flights involved, but basically all other combinations we have)

"""

#check the min values for clusters
def minvalue(data):
  minvalue = data[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice', 'distance']].min()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice'& 'distance': ")
  print(minvalue)

minvalue(cluster_1)
minvalue(cluster_2)
minvalue(cluster_3)
minvalue(cluster_4)

"""3d cluster has the highest min distance between OD, 4th the lowest"""

#investigate the group with respect to max value

def maxvalue(d):
  maxvalue = d[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice', 'distance']].max()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice' & 'distance': ")
  print(maxvalue)

maxvalue(cluster_1)
maxvalue(cluster_2)
maxvalue(cluster_3)
maxvalue(cluster_4)

"""Again distance and price for the 4th cluster, the range would be from 1 hour to 47 hours,0-36 waiting hours, 0-5 changes,0-0.745 walking distance, 0.72-50.840 for the price"""

#investigate the group with respect to mean value

def meanvalue(dat):
  meanvalue = dat[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice', 'distance']].mean()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice'& 'distance': ")
  print(meanvalue)

meanvalue(cluster_1)
meanvalue(cluster_2)
meanvalue(cluster_3)
meanvalue(cluster_4)

"""Fully the same pattern as it was before in the table, basically the centroids again -but unscaled somehow"""

#investigate the group with respect to median with sorted values in the middle

def medianvalue(dat):
  medianvalue = dat[['totaltraveltimeinhours', 'totalwaitingtime', 'totalnumberofchanges',	'totalwalkingdistance',	'totalprice', 'distance']].median()
  print("minimum value in column 'totalTravelTimeInHours' & 'totalWaitingTime & 'totalnumberofchanges & 'totalwalkingdistance & 'totalprice'& 'distance': ")
  print(medianvalue)

medianvalue(cluster_1)
medianvalue(cluster_2)
medianvalue(cluster_3)
medianvalue(cluster_4)

"""Repeats the mean pattern, expensive-third- cheapest-fourth"""

#get a 2 dimensional kernel density estimation (estimate of probability density) of all clusters using number of changes and distance OD

def plot1(data):
  plt.figure(figsize=(10,10))#check the size
  sns.kdeplot(x=data.totalnumberofchanges,y=data.distance,hue=data.travelfrom,shade=True)
  plt.title('Density investigation per clustered groups -distance/travelfrom')
  plt.xlabel('Transfers')
  plt.ylabel("Distance,km")

plot1(cluster_1)
plot1(cluster_2)
plot1(cluster_3)
plot1(cluster_4)

"""for the 2d and 3d group we are also travelling a lot from metropolis, travelling from metropolis encompasses lower distance than from big city

departing from metropolis we have less number of changes for the routes

the cheapest group has  metropolis departure only for 0-1 transfers
"""

#get a 2 dimensional kernel density estimation (estimate of probability density) of clusters using number of changes and waiting time since they have the degree of correlation

def plot2(data):
  plt.figure(figsize=(10,10))#check the size
  sns.kdeplot(x=data.totalnumberofchanges,y=data.totalwaitingtime,hue=data.travelto,shade=True)
  plt.title('Density estimation per clustered groups- waiting time/travelto')
  plt.xlabel('Transfers')
  plt.ylabel("Waiting time, hours")

plot2(cluster_1)
plot2(cluster_2)
plot2(cluster_3)
plot2(cluster_4)

"""1st non convenient cluster-with the highest waiting time when we travel to big city especially (when we travel to metropolis the waiting time is lower)

we travel to the small city only by non conveniete routes of 1st cluster with high travel,waiting time, we have 1 change

the cheapest group-travelling TO metropolis-normally waiting time is zero
"""

#check the distribution of traveltime per mode

def plot3(data):
  plt.figure(figsize=(20, 7))

  sns.stripplot(x=data['finalsolutionusedlabels'],y=data['totaltraveltimeinhours'],hue=data['finiteautomaton'],palette='YlGnBu')#configure the axis
  plt.xlabel("transport choices", size = 10)
  plt.xticks(rotation='vertical')
  plt.ylabel("travel time in hours", size = 10)
  plt.title('Distribution of travel time for the clustered groups with modes',fontsize=15)

plot3(cluster_1)
plot3(cluster_2)
plot3(cluster_3)
plot3(cluster_4)

"""1st group-mostly public transport,
2d group clearly no Flights,
3d expensive group includes all ofthe transport, especially having train would increase the travel time,
4th group noflight mode in general returns faster travel time than publictransportmode => almost no all types of transport involved
"""

#check the walking distance pattern per cities from where to travel

def plot4(data):
  plt.figure(figsize=(20, 7))

  sns.stripplot(x=data['travelfrom'],y=data['totalwalkingdistance'],hue=data['finiteautomaton'],palette='bwr')#configure the axis
  plt.xlabel("Origin", size = 10)
  plt.xticks(rotation='vertical')
  plt.ylabel("Walking distance,km", size = 10)
  plt.title('Distribution of distance by foot for the clustered groups per departures',fontsize=15)

plot4(cluster_1)
plot4(cluster_2)
plot4(cluster_3)
plot4(cluster_4)

"""2d group-less walks in comparison to other groups,with the mode noflights-walking distance still higher, travelling from bigcity the distance by foot to cover is greater,
1,2,4 groups- more walks travelling from big city then from metropolis
3d group same km to cover by foot from metropolis and bigcity-expensive group,
3d group-"all" transport modes gives the highest walking distance
"""

#check the walking distance pattern per cities where we are going
#use stripplot
def plot41(data):
  plt.figure(figsize=(20, 7))

  sns.stripplot(x=data['travelto'],y=data['totalwalkingdistance'],hue=data['finiteautomaton'],palette='bwr')#configure the axis
  plt.xlabel("Destination", size = 10)
  plt.xticks(rotation='vertical')
  plt.ylabel("Walking distance,km", size = 10)
  plt.title('Distribution of distance by foot for the clustered groups per arrivals',fontsize=15)

plot41(cluster_1)
plot41(cluster_2)
plot41(cluster_3)
plot41(cluster_4)

"""we have a small city added,
travelling TO big city-the highest walking distance, to metropolis the lowest except 3d expensive group
"""

#distribution of price per mode choice and preferences constraints
def plot5(dat):
  fig, axes = plt.subplots(1,2,figsize=(15,10))#unpack the tuple into the variables fig and axes

  plt.subplot(121)
  sns.boxenplot(data=dat, x='finiteautomaton',y='totalprice',palette='bright') #pick the inner parametres for plotting
  plt.title('Distribution of prices versus mode choise')
  plt.xlabel('Mode choice')
  plt.ylabel("Price")


  plt.subplot(122)
  sns.boxenplot(data=dat, x='consideredpreferences',y='totalprice',palette='bright')#pick the inner parametres for plotting
  plt.xticks(rotation='vertical')
  plt.title('Distribution of prices versus preference list')
  plt.xlabel('Preferences')
  plt.ylabel("Price")

plot5(cluster_1)
plot5(cluster_2)
plot5(cluster_3)
plot5(cluster_4)

"""Price is higher for pt only for all groups, and considering full number of constraints for all groups"""

#distribution of price travelling 'from' and 'to'
def plot51(dat):
  fig, axes = plt.subplots(1,2,figsize=(15,10))#unpack the tuple into the variables fig and axes

  plt.subplot(121)
  sns.boxenplot(data=dat, x='travelfrom',y='totalprice',palette='bright') #pick the inner parametres for plotting
  plt.title('Distribution of prices versus departure')
  plt.xlabel('Source')
  plt.ylabel("Price,euros")


  plt.subplot(122)
  sns.boxenplot(data=dat, x='travelto',y='totalprice',palette='bright')#pick the inner parametres for plotting
  plt.xticks(rotation='vertical')
  plt.title('Distribution of prices versus arrival')
  plt.xlabel('Destination')
  plt.ylabel("Price,euros")

plot51(cluster_1)
plot51(cluster_2)
plot51(cluster_3)
plot51(cluster_4)

"""For all groups more expensive to go from and arrive to big_city, to go to small city is the cheapest option"""

def plot6(data):
# top proposed solutions

  cnt_srs = data['finalsolutionusedlabels'].value_counts().nlargest(5) #the largest 5
  plt.figure(figsize=(15,10))
  sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="Greens_r")
  plt.title('Top 5 proposed transportation solutions per clustered group')
  plt.xticks(rotation='vertical')
  plt.xlabel('Final solution label of the algorithm per cluster', fontsize=12)
  plt.ylabel('Number of observations', fontsize=12)
  plt.show()

plot6(cluster_1)
plot6(cluster_2)
plot6(cluster_3)
plot6(cluster_4)

"""Each group is train as a top leader following by a car,except first the slowest one (non-convenient)-here is a combination=train+flixbus and the discrepancies beween other following modes are not that dramatic, we do not have pure @train@ in the first group in top 5 at all, only inside the combination"""

#setting the objects to category 
categ_data = clscaled.copy()
for i in categ_data.select_dtypes(include='object'):
    categ_data[i] = categ_data[i].astype('category')

#get the labels
proto_labs = kproto.labels_

#install a special library
pip install shap

#evaluation of our clusters- how were the groups?

from lightgbm import LGBMClassifier
import shap
from sklearn.model_selection import cross_val_score
#the subsample ratio of columns when constructing each tree
clf_kp = LGBMClassifier(colsample_by_tree=0.8)
cv_scores_kp = cross_val_score(clf_kp, categ_data, proto_labs, scoring='f1_weighted')
print(f'Cross validation F1 score for K-Prototypes clusters is {np.mean(cv_scores_kp)}')

"""the model produced the F1 score which is close to 1, that means that clusters were raised are distinguishable, the F1-score is the harmonic mean of precision and recall, the highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, our value is 0,969"""

clf_kp.fit(categ_data, proto_labs)#fitting

#explain which column influence on what group
explainer_kk = shap.TreeExplainer(clf_kp)
shap_values_kk = explainer_kk.shap_values(categ_data)

#plot the columns importance
shap.summary_plot(shap_values_kk, categ_data, plot_type="bar", plot_size=(20, 10))

"""Shap is a game theoretic approach to explain outcome of the ML models,distance was significant to push outcomes in 4th cluster (3d index) where we have the lowest distance as well as 3d cluster with the highest

finiteautomaton was significant to assign the values to groups for 2d cluster-we have noFlights as the prevail mode there

walkingdistance influenced on 1st cluster with the highest ratio of walking distance

traveltimeinhours contributed in 4th cluster (fastest transportation)

consideredpreferences column pushed to the second cluster with the low walking distance

first 5 columns were really meaningfull for clustering the dataset
"""

def plot7(data):
# invetsigate considered preferences
  cnt_srs = data['consideredpreferences'].value_counts().nlargest(3)#top 3
  plt.figure(figsize=(15,10))
  sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, palette="autumn")
  plt.title('Top 3 preferences as constraints per clustered group')
  plt.xticks(rotation='vertical')
  plt.xlabel('Preferences per cluster', fontsize=12)
  plt.ylabel('Number of observations', fontsize=12)
  plt.show()

plot7(cluster_1)
plot7(cluster_2)
plot7(cluster_3)
plot7(cluster_4)

"""Checked the 2d cluster and considered preferences

2d group 1st and 2d place-less preferences-less constraints in comparison to other groups
"""