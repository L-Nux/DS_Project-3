# -*- coding: utf-8 -*-
"""Distance+Travelfrom-to+Hopkins Github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lvUhjnh_njFMfWMGMiZNeDoEuVjyQ41s
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
#to include graphs next to the code
#useful libraries
import numpy as np #linear algebra
import math
import pandas as pd #data processing
import matplotlib.pyplot as plt #plotting
import seaborn as sns #visualization


file='withoutliersnew.csv'
df = pd.read_csv(file)
print(df.describe()) #statistics

df

# drop redundant column via reading
df.drop('Unnamed: 0', axis=1, inplace=True)

"""Feature incorporation-OD distance *numerical*"""

#import necessary libraries
import requests
import json
import numpy as np
import pandas as pd

#create arrays with cities
#trick-extend original array with also 29 pairs to have all result from googlemaps (instead of 26 for O)
departure = ['Essen', 'Heidelberg', 'Aachen', 'Bochum', 'Wuppertal', 'Würzburg',
       'Hagen', 'Erfurt', 'Solingen', 'Gelsenkirchen', 'Berlin',
       'Karlsruhe', 'Hamburg', 'Darmstadt', 'Dortmund', 'Stuttgart',
       'Nürnberg', 'München', 'Düsseldorf', 'Ulm, Germany', 'Köln', 'Mannheim',
       'Leipzig', 'Bonn', 'Oberhausen', 'Duisburg', 'Bamberg', 'Osnabrück', 'Erlangen']
destination = ['Gelsenkirchen', 'Oberhausen', 'Bonn', 'Duisburg', 'München',
       'Bochum', 'Solingen', 'Nürnberg', 'Karlsruhe', 'Wuppertal',
       'Hagen', 'Dortmund', 'Düsseldorf', 'Essen', 'Erlangen',
       'Osnabrück', 'Leipzig', 'Würzburg', 'Mannheim', 'Ulm, Germany', 'Bamberg',
       'Darmstadt', 'Köln', 'Stuttgart', 'Heidelberg', 'Erfurt', 'Berlin',
       'Aachen', 'Hamburg']
departure.sort() #sort the cities
destination.sort()

import numpy as np
import pandas as pd

#create matrix with departures and destinations
  
cities = np.zeros((len(departure), len(destination)))
#create new df
cities_df = pd.DataFrame(data = cities, 
                        index = departure, 
                        columns = destination)

#new function
def get_distance(A, B, mode):
  api_key = '...' # use googlemapsapikey in order to use their api to find the distances between 2 cities with defined mode
  url = "https://maps.googleapis.com/maps/api/distancematrix/json?origins=" + A + "&destinations=" + B + "&units=metric&mode=" + mode + "&key=" + api_key
  payload = {}
  headers = {}
  response = requests.request("GET", url, headers=headers, data=payload)
  try:
    distance = json.loads(response.text)
#parse json response from googlemaps api, get distance in km
    return distance['rows'][0]['elements'][0]['distance']['value'] / 1000
  except:
    return -1  #if gmaps doesnt return the values- indicate as -1 (mistake)
    #finding - Ulm returned mistake, by correcting 'Ulm, Germany' it was eliminated with -1

#test
print(get_distance('Hamburg', 'Erfurt', 'transit'))

#test function
print(get_distance(departure[0], destination[0], 'driving'))

#try out specific - default mode
#fill out cities dataframe
mode = 'driving'
for A in departure:
  for B in destination:
    if A != B:
      cities_df[A][B] = get_distance(A, B, mode) #obtain our distance in km
    else:
      cities_df[A][B] = "#"

#check new df
cities_df

#check the values
cities_df['Berlin']['München']

cities_df_transit = pd.DataFrame(data = cities, 
                        index = departure, 
                        columns = destination)

#try out different mode- transit
mode = 'transit'
for A in departure:
  for B in destination:
    if A != B:
      cities_df_transit[A][B] = get_distance(A, B, mode)
    else:
      cities_df_transit[A][B] = "#"

"""finally decided to pick transit mode for everything except car, since in the dataset we have a lot of public transport modes-train, flixbus- the most popular ones"""

#check different dataframe per another mode
cities_df_transit

dfnew= df.copy()
dfnew

#add a new numerical column 'distance' to dataset with nulls:
dfnew['distance'] = 0.0
dfnew

#replace Ulm by Ulm, Germany to transmit the values
dfnew['sourcename'] = dfnew['sourcename'].replace(['Ulm'],'Ulm, Germany')
dfnew['targetname'] = dfnew['targetname'].replace(['Ulm'],'Ulm, Germany')

#populate the dataframe
for i in range(0, len(dfnew)):
  if dfnew ["finalsolutionusedlabels"].values[i] == '[car]': #use 'driving' mode for the 'car' label
    dfnew['distance'][i] = cities_df[dfnew['sourcename'][i]][dfnew['targetname'][i]]
  else:
    dfnew['distance'][i] = cities_df_transit[dfnew['sourcename'][i]][dfnew['targetname'][i]] #use 'transit' mode for other labels

#check if we got the distances inserted in km
dfnew

"""correct- Essen	Gelsenkirchen	10.305 as in the dfmatrix above"""

#incorporate the dataset meta_city_size
file='metacitysize.csv'
dfcity = pd.read_csv(file)
print(dfcity.describe())

#explore the data
dfcity

"""we have metropolises, big cities and 1 small city"""

#create 2 new categorical columns for the dataset (features) -'travelfrom' and 'travelto'

dfnew['travelfrom'] = "null" #populate them with nulls at first
dfnew['travelto'] = "null"

dfnew

"""The values for these column would be based on size_category for city from the newest dataframe incorporated with data per town"""

#change value for alignment between 2 dataframes again after obtaining the distance
dfnew['sourcename'] = dfnew['sourcename'].replace(['Ulm, Germany'],'Ulm')
dfnew['targetname'] = dfnew['targetname'].replace(['Ulm, Germany'],'Ulm')

#populate our common dataframe based on aligment with another df regarding cities:
for i in range(0, len(dfnew)): #iterate through 1st dataset
  for j in range(0, len(dfcity)): #through 2d
    if dfnew ["sourcename"].values[i] == dfcity ["city"].values[j]:
      dfnew['travelfrom'].values[i] = dfcity ["size_category"].values[j]
    elif dfnew ["targetname"].values[i] == dfcity ["city"].values[j]:
      dfnew['travelto'].values[i] = dfcity ["size_category"].values[j]

#check if we are getting the correct features with the mode for driving

dfnew.loc[dfnew['finalsolutionusedlabels'] == '[car]']

#check the correctness for metropolis as source
dfnew.loc[dfnew['sourcename'] == 'Hamburg']

#check the correctness for small city as target
dfnew.loc[dfnew['targetname'] == 'Bamberg']

#save the newest df with 1 incorporated distance feature for further work
from google.colab import files
dfnew.to_csv('2featuresout.csv') 
files.download('2featuresout.csv')

#make a copy of data for Hopkins Statistics 
dfhopkins=dfnew.copy()

"""make a copy of data to perform Hopkins Statistics to decide on whether our data could be clustered or not, involvement of numerical columns"""

#drop categorical columns for the statistics
dfhopkins.drop(dfhopkins.columns[[5,6,7,8,9,10,12,13]], axis = 1, inplace = True)

#check the newest dataframe
dfhopkins

"""only numerical columns are inside"""

import sklearn 
from sklearn.preprocessing import StandardScaler
# to standardize values through scaler
scaler = StandardScaler()
# fit_transform
dfnew_scaled = scaler.fit_transform(dfhopkins)

#check the array
dfnew_scaled

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan
 
def hopkins(X):
    d = X.shape[1]
    #d = len(vars) # columns
    n = len(X) # rows
    m = int(0.1 * n) # heuristic from article [1]
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)
 
#generate a simulated data set (random D) drawn from a random uniform distribution with n points (q1,q2,…, qn) and the same variation as the original real data set D.
    rand_X = sample(range(0, n, 1), m)
 #compute the distance, xi, from each real point to each nearest neighbour: For each point, pi ∈ D, find it’s nearest neighbour pj; 
 #then, compute the distance between pi and pj and denote it as xi=dist(pi,pj)
    ujd = []
#compute the distance, yi from each artificial point to the nearest real data point: For each point qi ∈ random D, find it’s nearest neighbour qj in D; then compute the distance between qi and qj and denote it yi=dist(qi,qj)
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])
#calculate the Hopkins statistic (H) as the mean nearest neighbour distance in the random data set divided by the sum of the mean nearest neighbour distances in the real and across the simulated data set.
    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0
 
    return H

#convert “dfscaled” to pandas DataFrame & define previous column names 
dfnew_scaled = pd.DataFrame(dfnew_scaled)
dfnew_scaled.columns =['totaltraveltimeinhours','totalprice','totalnumberofchanges','totalwalkingdistance', 'totalwaitingtime', 'distance']

#apply Hopkins function to the dataset
hopkins(dfnew_scaled)

"""Apply Hopkins function to the dataset- the clusters should be meaningful considering the numerical columns.

A function "hopkins" compares scatter spread of data points from our dataset to random scatter data which contains no cluster tendency or properties,using NearestNeighbors, meaning that the test tells us how much percentage different is our data from random scatter data
We could apply Hopkins test before scaling or after the scaling as the scale of x-axis & y-axis changes- it does not affect the spread of points.
We conduct the Hopkins Statistic test iteratively, using 0.5 as the threshold to reject the alternative hypothesis.

If H < 0.5, then it is unlikely that data has statistically significant clusters.
If the value of Hopkins statistic is close to 1, then we can reject the null hypothesis and conclude that the dataset has significantly clusterable data with respect to the columns.

Our obtained value is extremely close to 1 even taking to consideration the adding of 1 additional feature-distance.
The numerical columns of dataframe are perfectly clusterable
"""